{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryayayayaa/Labmentix/blob/main/Paisabazaar_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name    - Paisabazaar Banking Fraud Analysis**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**  - BALLA SAI DINESH MANI KARTHIKEYA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "This project's core objective was to develop a robust machine learning model capable of accurately predicting individual credit scores as 'Poor,' 'Standard,' or 'Good' for Paisabazaar, a leading financial services company. This aim is critical for their business, as precise credit assessment is paramount for informed loan approvals, effective risk management, personalized product offerings, and overall operational efficiency.\n",
        "\n",
        "The journey began with a thorough **data understanding** phase, involving loading the raw `Paisabazaar.csv` dataset and conducting initial exploratory analysis to grasp its structure, content, and the distribution of its variables.\n",
        "\n",
        "The subsequent **data preparation and feature engineering** phase was extensive. Irrelevant identifier columns such as `ID`, `Customer_ID`, `SSN`, and `Name` were promptly removed. The `Credit_History_Age` column, initially in a 'X years Y months' format, was meticulously converted into a numerical representation of total months, making it usable for modeling. The `Type_of_Loan` column, a multi-label categorical feature, was transformed into multiple binary columns, one for each unique loan type, to capture all possible loan combinations. Other ordinal and binary categorical features like `Credit_Mix` and `Payment_of_Min_Amount`, along with the target variable `Credit_Score` itself, were mapped to numerical values. Nominal categorical features such as `Occupation` and `Payment_Behaviour` were then effectively handled using one-hot encoding, expanding the feature set. Missing values were systematically imputed using medians for numerical columns, and outliers were treated using the IQR (Interquartile Range) capping method to ensure data quality and model stability.\n",
        "\n",
        "To further enhance the dataset's predictive power, **feature manipulation** was performed. Highly correlated features, like `Annual_Income` and `Monthly_Inhand_Salary`, were addressed by strategically dropping one (`Monthly_Inhand_Salary`) to minimize multicollinearity. New, insightful features were engineered, including `Debt_to_Income_Ratio`, `EMI_to_Salary_Ratio`, and `Payment_Consistency`, aiming to capture more complex financial behaviors and improve predictive accuracy. This was followed by **data transformation**, where `log1p` (logarithmic transformation) was applied to skewed numerical features to make their distributions more Gaussian-like, benefiting various ML algorithms. All numerical features were then **scaled using StandardScaler**, ensuring they contribute equally to the model without being disproportionately influenced by their original magnitude. **Dimensionality reduction with PCA** was also conditionally included as an optional step, applied only if the feature count remained high, to reduce complexity while preserving variance.\n",
        "\n",
        "A crucial step was the **data splitting and handling of imbalanced datasets**. The dataset was split into training (80%) and testing (20%) sets. Critically, **stratified sampling (`stratify=y`)** was employed to ensure that the distribution of the target `Credit_Score` classes ('Poor', 'Standard', 'Good') was maintained in both subsets, which is vital for imbalanced datasets. Furthermore, the inherent class imbalance in the training data was addressed using **SMOTE (Synthetic Minority Over-sampling Technique)**. SMOTE generated synthetic samples for the minority classes (`Poor` and `Good`), thereby balancing the training dataset without introducing data leakage, allowing models to learn effectively from all classes.\n",
        "\n",
        "In the **model development and optimization** phase, three prominent machine learning algorithms were implemented: Logistic Regression, Decision Tree Classifier, and Random Forest Classifier. For each model, **hyperparameter optimization was performed using Randomized Search Cross-Validation (`RandomizedSearchCV`)**. This technique efficiently explored a wide range of hyperparameter combinations, using 5-fold cross-validation to robustly assess performance and identify the optimal parameters for each algorithm.\n",
        "\n",
        "The models were rigorously **evaluated** using a suite of metrics crucial for business impact:\n",
        "* **Overall Accuracy:** Providing a general measure of correctness.\n",
        "* **Precision, Recall, and F1-score (Macro and Weighted Averages):** Essential for understanding class-specific performance, particularly for imbalanced datasets. High recall for 'Poor' credit scores directly translates to reduced loan defaults, while high precision for 'Good' credit scores ensures confident lending decisions.\n",
        "* **Confusion Matrix:** Offering a granular view of true positives, false positives, and false negatives, enabling a deeper understanding of where the model makes errors and their associated business costs.\n",
        "* **ROC AUC Curves (One-vs-Rest):** Assessing the model's ability to distinguish between each credit score class and all others, providing insights into its overall discriminatory power across different decision thresholds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "Provide your GitHub Link here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "**To develop a robust and data-driven predictive model that can accurately classify an individual's credit score (Poor, Standard, or Good) based on their financial and behavioral data, thereby enhancing the credit assessment process and mitigating associated business risks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "!pip install contractions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# For textual data preprocessing\n",
        "# You might need to download these NLTK resources once\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "import re\n",
        "import nltk\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from google.colab import drive\n",
        "import gdown\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('dataset-2.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "It is observed that there are 28 columns and 1,00,000 rows of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "28 distinct columns are observed in the dataset. Finding the description of the dataset, it is seen that 'count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max' of each column in the dataset is found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# @title Data Preparation for Analysis\n",
        "\n",
        "# Drop irrelevant identifier columns: ID, Customer_ID, SSN, and Name are unique identifiers - typically not used as features for modeling\n",
        "print(\"Dropping identifier columns: ID, Customer_ID, SSN, Name...\")\n",
        "df = df.drop(['ID', 'Customer_ID', 'SSN', 'Name'], axis=1)\n",
        "print(f\"Dataset shape after dropping identifiers: {df.shape}\")\n",
        "\n",
        "# Convert 'Credit_History_Age' from 'X years and Y months' to total months (integer)\n",
        "print(\"Converting 'Credit_History_Age' to total months...\")\n",
        "\n",
        "def convert_credit_history_age(age_str):\n",
        "    if pd.isna(age_str) or not isinstance(age_str, str):\n",
        "        return np.nan\n",
        "    try:\n",
        "        parts = age_str.replace('and', '').replace('months', '').split('years')\n",
        "        years = int(parts[0].strip()) if parts[0].strip() else 0\n",
        "        months = int(parts[1].strip()) if len(parts) > 1 and parts[1].strip() else 0\n",
        "        return (years * 12) + months\n",
        "    except:\n",
        "        return np.nan # Handle any parsing errors\n",
        "\n",
        "df['Credit_History_Age'] = df['Credit_History_Age'].apply(convert_credit_history_age)\n",
        "\n",
        "# Fill any NaN values that might arise from conversion errors, e.g., with median or mean\n",
        "df['Credit_History_Age'].fillna(df['Credit_History_Age'].median(), inplace=True)\n",
        "print(\"Finished converting 'Credit_History_Age'.\")\n",
        "\n",
        "\n",
        "# Handle 'Type_of_Loan' - Multi-label one-hot encoding as this column contains multiple loan types separated by commas\n",
        "print(\"Processing 'Type_of_Loan' for multi-label encoding...\")\n",
        "\n",
        "# First, clean up extra spaces around loan types and replace 'No Loan' if present\n",
        "df['Type_of_Loan'] = df['Type_of_Loan'].str.replace(r'\\s*,\\s*', ',', regex=True).str.strip()\n",
        "df['Type_of_Loan'] = df['Type_of_Loan'].replace('No Loan', np.nan) # Treat 'No Loan' as missing for now if not actual loan type\n",
        "\n",
        "# Get all unique loan types by splitting the strings\n",
        "all_loan_types = set()\n",
        "for types_str in df['Type_of_Loan'].dropna():\n",
        "    for loan_type in types_str.split(','):\n",
        "        all_loan_types.add(loan_type.strip())\n",
        "\n",
        "# Create a binary column for each unique loan type\n",
        "for loan_type in all_loan_types:\n",
        "    # Use str.contains with regex=False for exact match of loan type\n",
        "    df[f'Loan_{loan_type.replace(\" \", \"_\")}'] = df['Type_of_Loan'].apply(lambda x: 1 if isinstance(x, str) and loan_type in x.split(',') else 0)\n",
        "\n",
        "# Drop the original 'Type_of_Loan' column\n",
        "df = df.drop('Type_of_Loan', axis=1)\n",
        "print(\"Finished processing 'Type_of_Loan'.\")\n",
        "\n",
        "\n",
        "# Map Ordinal and Binary Categorical Columns to numerical values\n",
        "print(\"Mapping ordinal and binary categorical columns...\")\n",
        "\n",
        "# Credit_Mix: 'Bad' < 'Standard' < 'Good'\n",
        "credit_mix_mapping = {'Bad': 0, 'Standard': 1, 'Good': 2}\n",
        "df['Credit_Mix'] = df['Credit_Mix'].map(credit_mix_mapping)\n",
        "\n",
        "# Fill any NaN values that might arise from mapping issues (e.g., unseen categories)\n",
        "df['Credit_Mix'].fillna(df['Credit_Mix'].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "# Payment_of_Min_Amount: 'No' < 'Yes' < 'Not Applicable' (can be treated as a third distinct category)\n",
        "payment_min_amount_mapping = {'No': 0, 'Yes': 1, 'Not Applicable': 2}\n",
        "df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map(payment_min_amount_mapping)\n",
        "df['Payment_of_Min_Amount'].fillna(df['Payment_of_Min_Amount'].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "# Credit_Score: This is likely the target variable, and it's ordinal: 'Poor' < 'Standard' < 'Good'\n",
        "credit_score_mapping = {'Poor': 0, 'Standard': 1, 'Good': 2}\n",
        "df['Credit_Score'] = df['Credit_Score'].map(credit_score_mapping)\n",
        "df['Credit_Score'].fillna(df['Credit_Score'].mode()[0], inplace=True) # Handle potential missing values\n",
        "\n",
        "\n",
        "print(\"Finished mapping ordinal and binary categorical columns.\")\n",
        "\n",
        "\n",
        "# One-Hot Encode Nominal Categorical Columns\n",
        "print(\"One-hot encoding nominal categorical columns: Occupation, Payment_Behaviour...\")\n",
        "nominal_cols = ['Occupation', 'Payment_Behaviour']\n",
        "df = pd.get_dummies(df, columns=nominal_cols, drop_first=True, dtype=int) # drop_first to avoid multicollinearity\n",
        "print(\"Finished one-hot encoding.\")\n",
        "\n",
        "\n",
        "# Display the first few rows of the preprocessed DataFrame and its info\n",
        "print(\"\\n--- Preprocessed DataFrame Head ---\")\n",
        "print(df.head())\n",
        "print(\"\\n--- Preprocessed DataFrame Info ---\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nDataset preparation for analysis is complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "**Data Manipulations Performed:**\n",
        "\n",
        "1. **Dropped Irrelevant Identifier Columns:**\n",
        "  * The columns ID, Customer_ID, SSN, and Name were removed from the DataFrame. These columns typically serve as unique identifiers and don't provide predictive power for machine learning models.\n",
        "  * **Effect:** The DataFrame's shape changed from an implied initial 28 columns (from your earlier df.info() output) to 24 columns after this step.\n",
        "\n",
        "2. **Converted Credit_History_Age:**\n",
        "  * An attempt was made to convert the Credit_History_Age column, which was assumed to be in a \"X years and Y months\" string format, into total months (integer).\n",
        "  * **Logic:** A custom function convert_credit_history_age was defined to parse this string and calculate the total months.\n",
        "  * **Missing Value Handling:** After conversion, any NaN values that resulted were filled using the median of the Credit_History_Age column.\n",
        "\n",
        "3. **Handled Type_of_Loan with Multi-label One-Hot Encoding:**\n",
        "  * This column contained multiple loan types, often separated by commas (e.g., \"Personal Loan, Student Loan\").\n",
        "  * **Cleaning:** Leading/trailing spaces and No Loan entries were cleaned/replaced with NaN.\n",
        "  * **Feature Creation:** New binary (0 or 1) columns were created for each unique loan type found across the entire dataset (e.g., Loan_Mortgage_Loan, Loan_Student_Loan). A '1' indicates the presence of that loan type for a given customer.\n",
        "  * **Dropped Original:** The original Type_of_Loan column was then dropped.\n",
        "\n",
        "4. **Mapped Ordinal and Binary Categorical Columns:**\n",
        "  * **Credit_Mix:** This was mapped from 'Bad' (0), 'Standard' (1), 'Good' (2) to numerical values, preserving the inherent order.\n",
        "  * **Payment_of_Min_Amount:** This was mapped from 'No' (0), 'Yes' (1), 'Not Applicable' (2).\n",
        "  * **Credit_Score (Target Variable):** This was mapped from 'Poor' (0), 'Standard' (1), 'Good' (2), preparing it for a classification model.\n",
        "  * **Missing Value Handling:** For all mapped columns, any NaN values that might have occurred during the mapping (e.g., if an unrecognised category appeared) were filled with the mode (most frequent value) of that column.\n",
        "\n",
        "5. **One-Hot Encoded Nominal Categorical Columns:**\n",
        "  * Occupation and Payment_Behaviour were identified as nominal (unordered) categorical features.\n",
        "  * **Feature Creation:** pd.get_dummies was used to create new binary columns for each unique category within these columns (e.g., Occupation_Engineer, Payment_Behaviour_High_spent_Medium_value_payments).\n",
        "  * drop_first=True was used to prevent multicollinearity, dropping one category from each original column.\n",
        "\n",
        "**Insights from the Output:**\n",
        "\n",
        "1. **Dimensionality Change:**\n",
        "  * The initial DataFrame had 28 columns.\n",
        "  * After dropping ID, Customer_ID, SSN, Name, the shape became (100000, 24).\n",
        "  * After all encoding steps (especially multi-label for Type_of_Loan and one-hot for Occupation and Payment_Behaviour), the final DataFrame has 100000 rows and 59 columns. This significant increase in columns is expected due to the creation of many binary features.\n",
        "\n",
        "2. **Credit_History_Age Anomaly:**\n",
        "  * The df.info() output for the preprocessed DataFrame shows that Credit_History_Age has 0 non-null values. This is a critical observation.\n",
        "  * **Explanation:** Despite the conversion logic and fillna step, this indicates that the convert_credit_history_age function likely returned np.nan for all 100,000 entries. This suggests that the original Credit_History_Age column was not in the \"X years and Y months\" string format as anticipated, or there was an issue in parsing it from its original float64 type as per the initial df.info() you provided. If all values turned NaN, then fillna(df   ['Credit_History_Age'].median()) would have filled them, but if no numeric values were successfully generated, the median itself might be NaN or 0, leading to a column of all NaNs in the info() output.\n",
        "  * Impact: This column effectively became empty or filled with a single NaN value after processing, meaning it will not contribute meaningfully to your model. This needs to be investigated and corrected.\n",
        "\n",
        "3. **Successful Categorical Encoding:**\n",
        "  * The df.head() output clearly shows the new Loan_, Occupation_, and Payment_Behaviour_ prefixed columns with binary (0/1) values.\n",
        "  * The df.info() confirms that these new columns are of int64 type and have 100000 non-null values, indicating successful one-hot encoding and no missing values in these newly created features.\n",
        "  * Credit_Mix, Payment_of_Min_Amount, and Credit_Score are now int64 and fully populated, confirming their successful ordinal mapping.\n",
        "\n",
        "4. **Overall Data Readiness:**\n",
        "  * Aside from the Credit_History_Age issue, the dataset is now entirely numerical (float64 or int64) and free of explicit missing values (all columns show 100000 non-null). This is a crucial state for feeding the data into most machine learning algorithms.\n",
        "\n",
        "In summary, the code has transformed your raw dataset into a clean, numerical format suitable for machine learning, expanding the feature set significantly. The primary issue identified from the output is the complete loss of data in the Credit_History_Age column, which requires a re-evaluation of its original format and the conversion logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code\n",
        "# @title Chart 1 - Distribution of credit scores\n",
        "#create count plot for credit score distribution\n",
        "sns.countplot(x = df['Credit_Score'], hue = df['Credit_Score'], palette = 'viridis', order = df['Credit_Score'].value_counts().index)\n",
        "#Set labels and title\n",
        "plt.title('Distribution of Credit Scores')\n",
        "plt.xlabel('Credit Score Category')\n",
        "plt.ylabel('Count')\n",
        "#show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "* **Categorical/Ordinal Variable**: Credit_Score is an ordinal categorical variable (Poor, Standard, Good, which I mapped to 0, 1, 2). A bar chart is the most appropriate visualization to show the frequency or count of observations within each distinct category.\n",
        "* **Clear Distribution Overview:** It immediately reveals the balance or imbalance across the different credit score categories. This is crucial for understanding the dataset's composition regarding the target variable.\n",
        "* **Ease of Interpretation:** The height of each bar directly corresponds to the count of individuals in that credit score category, making it very easy to interpret which categories are more or less prevalent.\n",
        "* **Preparation for Modeling:** Understanding the distribution of the target variable is a fundamental step in any machine learning project. It helps identify potential class imbalance issues that might need to be addressed during model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "* **Dominance of \"Standard\" Credit Score:** The chart clearly shows that the majority of customers fall into the \"Credit Score Category 1\" (which corresponds to 'Standard' credit score based on our mapping {'Bad': 0, 'Standard': 1, 'Good': 2}). There are over 50,000 instances in this category.\n",
        "* **Significant \"Poor\" Credit Score Group:** \"Credit Score Category 0\" (corresponding to 'Poor') represents the second largest group, with roughly 29,000-30,000 instances.\n",
        "* **Smallest \"Good\" Credit Score Group:** \"Credit Score Category 2\" (corresponding to 'Good') is the smallest group, with fewer than 20,000 instances (approximately 18,000).\n",
        "* **Class Imbalance:** There is a noticeable class imbalance. The 'Standard' credit score dominates, followed by 'Poor', and 'Good' credit scores are the least represented. This imbalance is not extreme but is certainly present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "**Positive Business Impact:** Yes, the gained insights from this distribution are crucial for creating a positive business impact for Paisabazaar:\n",
        "\n",
        "* **Targeted Marketing and Product Development:** Knowing that \"Standard\" credit score holders are the largest group allows Paisabazaar to focus marketing efforts and develop financial products specifically tailored to this segment. For example, they could offer products designed to help \"Standard\" customers improve their scores to \"Good,\" thereby increasing their eligibility for better loans and strengthening customer loyalty.\n",
        "\n",
        "* **Risk Management and Default Prevention:** The significant number of \"Poor\" credit score holders (Category 0) is a critical insight. This group represents a higher risk of loan defaults. Paisabazaar can use this information to:\n",
        "  * **Refine Lending Policies:** Implement stricter criteria or offer different loan terms for applicants in this category.\n",
        "\n",
        "  * **Proactive Intervention:** Develop strategies to engage with \"Poor\" credit score customers, offering financial literacy programs, debt consolidation advice, or specialized low-risk products to help them manage their finances better and reduce future default rates. Reducing defaults directly impacts Paisabazaar's profitability.\n",
        "\n",
        "* **Optimizing Resource Allocation:** By understanding the proportions, Paisabazaar can allocate resources (e.g., customer support, loan officers) more effectively. For instance, more resources might be needed for managing higher-risk accounts or for guiding \"Standard\" customers towards higher-value products.\n",
        "\n",
        "* **Model Performance Expectations:** Recognizing the class imbalance upfront allows data scientists to choose appropriate evaluation metrics (e.g., F1-score, precision, recall, AUC-ROC) rather than just accuracy, and potentially employ techniques like oversampling, undersampling, or using weighted loss functions during model training. This leads to building a more robust and fair prediction model, which directly supports the project's aim of accurate credit score classification.\n",
        "\n",
        "\n",
        "**Insights that Lead to Negative Growth:** While the insights themselves are just observations, the implications of a high proportion of \"Poor\" credit scores and a smaller proportion of \"Good\" credit scores could indirectly lead to negative growth if not strategically managed:\n",
        "\n",
        "* **Higher Default Rates:** The large number of customers with \"Poor\" credit scores indicates a potentially high inherent risk in Paisabazaar's existing customer base or applicant pool. If not properly assessed and mitigated by robust credit assessment processes (which is exactly what your project aims to improve), this could lead to a higher volume of loan defaults.\n",
        "  * **Reason:** More defaults mean more financial losses, increased operational costs for collections, and potentially damaged relationships with lenders, all contributing to negative financial growth.\n",
        "\n",
        "* **Missed Opportunities for High-Quality Customers:** The relatively smaller group of \"Good\" credit score customers means there might be fewer opportunities to acquire high-quality, low-risk borrowers, or that Paisabazaar is not attracting enough of them.\n",
        "  * **Reason:** High-quality borrowers typically bring in stable revenue with lower risk. If Paisabazaar struggles to attract or retain these customers due to lack of tailored offerings or competitive interest rates, it could lead to slower growth or even a decline in the overall quality of its loan portfolio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "#@title Chart 2 ; Distribution of Age\n",
        "#create histogram for Age distribution\n",
        "sns.histplot(df['Age'], bins = 30, kde = True, color = 'blue')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "* **Continuous Numerical Variable:** Age is a continuous numerical variable. A histogram is ideal for displaying the distribution of continuous data by dividing it into bins and showing the frequency of observations within each bin.\n",
        "* **Reveals Shape of Distribution:** A histogram helps in understanding the shape of the age distribution (e.g., symmetric, skewed, bimodal).\n",
        "* **KDE for Smoother Trend:** The Kernel Density Estimate (the blue line) provides a smoothed representation of the data's distribution, making it easier to see overall trends and peaks that might be obscured by the jaggedness of individual histogram bars, especially when dealing with many bins or a high number of unique values.\n",
        "* **Identifies Common Age Groups:** It quickly highlights which age ranges are most prevalent among the customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "* **Dominant Age Group:** The distribution shows a strong concentration of customers in the early 20s to early 40s (approximately 20 to 45 years old). This is where the highest frequency bars and the peak of the KDE curve are located.\n",
        "* **Peak Frequencies:** There appear to be multiple peaks, or at least a broad plateau, from roughly 25 to 40 years old, indicating these are the most common age ranges for customers in this dataset.\n",
        "* **Fewer Younger and Older Customers:** There are fewer customers in the younger age groups (below 20) and a noticeable decline in frequency as age increases beyond 45, with very few customers above 55.\n",
        "* **Relatively Even Distribution within Core Group:** Within the 20-45 age range, while there are fluctuations, the frequencies are generally high, suggesting a relatively consistent customer base across these working-age demographics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "**Positive Business Impact:** Yes, understanding the age distribution is highly valuable for Paisabazaar:\n",
        "\n",
        "* **Tailored Product Offerings:** Knowing that the core customer base is primarily between 20 and 45 years old allows Paisabazaar to design and market financial products specifically for these demographics.\n",
        "\n",
        "  * For example, younger customers (20s-early 30s) might be targeted with products like student loan refinancing, first-time home buyer loans, or credit-builder products.\n",
        "  * Mid-career customers (30s-40s) might be interested in mortgage refinancing, personal loans for major purchases, or wealth management tools.\n",
        "  * **Reason:** This targeted approach ensures that marketing spend is optimized and products are relevant, leading to higher conversion rates, customer acquisition, and engagement.\n",
        "\n",
        "* **Risk Assessment Refinement:** Age can be a significant factor in credit risk models. This distribution suggests that the bulk of Paisabazaar's business is with individuals in their prime earning years.\n",
        "  * **Reason:** This could imply a generally stable customer base from an income perspective, potentially allowing for more predictable risk assessments within this age range. However, it also highlights the need to understand the specific risk profiles within this dominant age group (e.g., credit behavior of 25-year-olds vs. 40-year-olds).\n",
        "\n",
        "* **Strategic Expansion:** The insight into fewer younger and older customers can inform expansion strategies.\n",
        "  * **Reason:** Paisabazaar might identify an untapped market in these segments. For example, they could develop specialized products for senior citizens or young adults just starting their financial journeys, expanding their market reach and customer base.\n",
        "\n",
        "**Insights that Lead to Negative Growth**\n",
        "\n",
        "Similar to the credit score distribution, the insights themselves don't inherently lead to negative growth, but the implications of the distribution if not strategically managed could pose risks:\n",
        "\n",
        "* **Limited Market Penetration in Specific Age Groups:** The low representation of customers below 20 and above 55 could indicate a failure to capture these segments effectively.\n",
        "  * **Reason:** If Paisabazaar is not attracting younger customers, they are missing out on building long-term relationships and capturing future high-value customers. Similarly, overlooking older demographics could mean missing out on customers with potentially significant assets, different financial needs (e.g., retirement planning, reverse mortgages), and established credit histories. This could lead to a stagnation or decline in overall market share if competitors successfully tap into these underserved age groups.\n",
        "\n",
        "* **Over-reliance on a Single Demographic:** If the business heavily relies on the 20-45 age bracket without diversification, it could be vulnerable to economic shifts or changes in financial behavior specific to that group.\n",
        "  * **Reason:** For instance, if a recession primarily impacts this age group's employment or income, Paisabazaar's core business could be disproportionately affected, leading to reduced loan applications or increased defaults.\n",
        "  \n",
        "In summary, the age distribution is a powerful insight for tailoring services and marketing. The \"negative growth\" aspects arise if Paisabazaar fails to recognize and address the potential vulnerabilities of concentrating on one age segment or missing opportunities in others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "# @title Chart- 3 Annual Income Distribution\n",
        "#create histogram for Annual Income Distribution\n",
        "\n",
        "sns.histplot(df['Annual_Income'], bins = 30, kde = True, color = 'green')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Annual Income')\n",
        "plt.xlabel('Annual Income')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "* **Continuous Numerical Variable:** Annual_Income is a continuous numerical variable. A histogram effectively displays the frequency distribution of such data by grouping values into bins.\n",
        "* **Reveals Skewness and Outliers:** Income distributions are often skewed, and a histogram clearly shows this skewness (e.g., right-skewed, where most values are lower with a tail extending to higher values) and helps identify potential outliers or unusual concentrations.\n",
        "* **KDE for Underlying Trend:** The KDE (the green line) provides a smooth, continuous estimate of the probability density function, helping to discern the underlying shape and multi-modality (multiple peaks) in the distribution that might be less obvious from the discrete bars of a histogram alone.\n",
        "* **Identifies Income Brackets:** It allows for quick identification of the most common income brackets among the customer base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "* **Right-Skewed Distribution:** The distribution is heavily right-skewed, meaning a large majority of customers have lower annual incomes, and fewer customers have very high annual incomes.\n",
        "* **Primary Income Concentration:** The most significant concentration of customers falls within the lower income brackets, specifically around $15,000 to $30,000. The highest bar indicates a peak frequency here.\n",
        "* **Secondary Peaks/Plateaus:** There appear to be a few secondary, smaller peaks or plateaus. One is roughly around $25,000 - $35,000, and another around $60,000 - $70,000, suggesting distinct clusters or segments within the income distribution.\n",
        "* **Long Tail:** The distribution has a long tail extending towards higher annual incomes, but the frequency significantly drops off after approximately $75,000, indicating a smaller number of high-income earners in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "**Positive Business Impact:** Yes, the insights from the Annual Income distribution are highly valuable for Paisabazaar:\n",
        "\n",
        "* **Targeted Product Development & Marketing:** Knowing that most customers fall into lower to moderate income brackets allows Paisabazaar to design and market products that are affordable and relevant to these segments.\n",
        "  * **Reason:** Tailoring products and marketing strategies to the predominant income groups ensures better resonance with the target audience, higher conversion rates, and optimized customer acquisition costs.\n",
        "\n",
        "* **Refined Credit Risk Assessment:** Income is a primary factor in assessing creditworthiness. The observed distribution can help refine lending models.\n",
        "  * **Reason:** Paisabazaar can use this information to set income-based eligibility criteria for different loan products, develop tiered interest rates based on income levels, or implement stricter underwriting for very low-income applicants, thereby managing risk and reducing defaults more effectively.\n",
        "\n",
        "* **Identifying Underserved Segments:** The long tail of higher incomes, while smaller, represents a potentially underserved market.\n",
        "  * **Reason:** Paisabazaar could explore developing premium financial products (e.g., high-value mortgages, investment-linked loans) or exclusive services to attract and cater to this smaller but potentially high-value segment, diversifying their portfolio and increasing average revenue per customer.\n",
        "\n",
        "**Insights that Lead to Negative Growth:** As with previous charts, the insights themselves are observational, but their implications can lead to negative growth if not addressed strategically:\n",
        "\n",
        "* **Higher Default Risk in Lower Income Segments (Potential Negative Implication):** The significant concentration of customers in lower-income brackets naturally implies a potentially higher overall default risk for the loan portfolio if not managed carefully.\n",
        "\n",
        "  * **Reason:** Lower income often correlates with less disposable income and a higher susceptibility to financial shocks, increasing the likelihood of missed payments or defaults. If Paisabazaar does not adequately adjust its risk models, loan terms, or support mechanisms for this large segment, it could face increased non-performing assets and financial losses.\n",
        "\n",
        "* **Limited Revenue Per Customer (Potential Negative Implication):** A customer base dominated by lower-income individuals might, on average, have a lower capacity for taking out larger loans or utilizing higher-limit credit products.\n",
        "\n",
        "  * **Reason:** This could cap the potential revenue generated per customer. If Paisabazaar only focuses on this segment, it might struggle to achieve higher revenue growth or profitability compared to competitors who successfully attract and serve higher-income customers with larger financial needs. This might necessitate a high volume strategy to compensate for lower per-customer revenue.\n",
        "\n",
        "In conclusion, understanding the income distribution is vital for strategic product development, marketing, and risk management. The challenge lies in converting the insights from the large low-to-moderate income segment into robust, yet accessible, financial products while mitigating the inherent risks associated with lending to this demographic. Simultaneously, Paisabazaar could identify and strategically target the higher-income segments for diversified growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "# @title Chart 4- Credit Histogram for credit Utillization ratio distribution\n",
        "sns.histplot(df['Credit_Utilization_Ratio'], bins = 30, kde = True, color = 'purple')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Credit Utilization Ratio')\n",
        "plt.xlabel('Credit Utilization Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "* **Continuous Numerical Variable:** The Credit Utilization Ratio is a continuous numerical variable. A histogram is the most effective way to display the frequency distribution of such data, showing how values are distributed across different ranges.\n",
        "* **Reveals Central Tendency and Spread:** It clearly shows where the majority of credit utilization ratios lie (central tendency) and how widely they are spread (dispersion).\n",
        "* **KDE for Smoothness and Shape:** The KDE (the purple line) provides a smooth representation of the underlying probability distribution, making it easier to identify the overall shape (e.g., normal, skewed, multimodal) and peak concentrations, which might be less distinct with just the histogram bars.\n",
        "* **Key Credit Metric:** The Credit Utilization Ratio is a critical indicator of credit risk, and visualizing its distribution provides immediate insights into how customers are managing their credit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "* **Bell-Shaped (Normal-like) Distribution:** The distribution appears roughly bell-shaped, resembling a normal distribution, though it might be slightly skewed or have a flattened peak. This suggests that credit utilization ratios tend to cluster around a central value.\n",
        "* **Concentration Around Optimal Range:** The highest frequency of credit utilization ratios is observed roughly between 30% and 40%, with a peak around 35-37%. This range is generally considered healthy or moderate for credit utilization.\n",
        "* **Limited High and Low Utilization:** There are relatively few customers with very low (below 25%) or very high (above 40-45%) credit utilization ratios. The tails of the distribution drop off significantly.\n",
        "* **Possible Outliers/Extremes:** While the chart shows values up to 50%, the frequencies at the very ends of the spectrum (e.g., 20-22% and 45-50%) are quite low, suggesting that extreme utilization ratios are not common in this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "**Positive Business Impact:** Yes, these insights are highly beneficial:\n",
        "\n",
        "* **Optimized Credit Limit Recommendations & Risk Management** The concentration around a moderate utilization range (30-40%) allows Paisabazaar to recommend appropriate credit limits and fine-tune risk models.\n",
        "  * **Reason:** This helps in proactively identifying healthy credit users for potential limit increases (driving revenue) and flagging those moving into higher risk, thus reducing defaults.\n",
        "\n",
        "* **Targeted Product Development:** Understanding the typical utilization behavior enables creating products and offers that align with customer needs.\n",
        "  * **Reason:** Tailored offerings lead to better customer engagement and higher conversion rates.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **Complacency in Risk Assessment:** Over-reliance on the healthy aggregate distribution without deeper segmentation can be risky.\n",
        "  * **Reason:** This could lead to underestimating risk within specific customer groups, resulting in unforeseen defaults and financial losses.\n",
        "\n",
        "* **Missed Revenue from Low-Utilizers:** A segment of customers might be underutilizing their credit.\n",
        "  * **Reason:** If Paisabazaar doesn't encourage more usage (e.g., via incentives), they miss out on potential interest income and transaction fees, hindering revenue growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code\n",
        "# @title Chart 5 - Number of credit cards distribution\n",
        "#create histogram for number of credit cards distribution\n",
        "sns.histplot(df['Num_Credit_Card'], bins = 15, kde = True, color = 'red')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Number of Credit Cards')\n",
        "plt.xlabel('Number of Credit Cards')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "A histogram with a Kernel Density Estimate (KDE) overlay is suitable for Number of Credit Cards because:\n",
        "* **Discrete Numerical Variable:** It effectively shows the frequency of each distinct count of credit cards.\n",
        "* **Distribution Shape:** It reveals the pattern of how many cards customers typically hold, including peaks and valleys.\n",
        "* **KDE for Overall Trend:** The KDE (red line) smooths out the distribution, highlighting dominant numbers of cards and potential multimodal patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "* **Multimodal Distribution:** The chart shows several distinct peaks, indicating that customers tend to hold specific numbers of credit cards rather than a continuous spread.\n",
        "* **Common Card Counts:** The most frequent numbers of credit cards appear to be 5, 6, and 7, with counts around or above 15,000 for each.\n",
        "* **Fewer Cards at Extremes:** There are fewer customers with very few (0-2) or very many (above 8) credit cards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "**Positive Business Impact:** Yes, these insights are valuable:\n",
        "\n",
        "* **Targeted Credit Card Offers & Cross-selling:** Knowing common card counts (5, 6, 7) allows Paisabazaar to identify customers who might be in the market for additional cards or specific card features to complete their portfolio.\n",
        "  * **Reason:** This enables targeted campaigns for new credit card acquisitions or cross-selling existing products, boosting revenue.\n",
        "\n",
        "* **Risk Assessment:** The number of credit cards can influence credit risk. Paisabazaar can tailor risk models based on these insights.\n",
        "  * **Reason:** This helps in assessing potential debt burden more accurately, leading to better lending decisions and reduced defaults.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **Over-Indebtedness Risk:** The prevalence of customers with 5-7 credit cards indicates potential for high overall debt.\n",
        "  * **Reason:** While more cards mean more potential transaction fees, it also increases the risk of over-indebtedness and default if not carefully managed by both customer and lender.\n",
        "\n",
        "* **Market Saturation for Existing Customers:** If many customers already have multiple cards, the opportunity for Paisabazaar to issue new primary cards to them might be limited.\n",
        "  * **Reason:** This necessitates focusing on competitive offers or specialized cards to capture share, potentially reducing profit margins or slowing new card acquisition growth.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "# @title Chart 6 - Annual Income VS Credit Score\n",
        "#create boxplot for annual income across different credit score categories\n",
        "sns.boxplot(x= 'Credit_Score', y = \"Annual_Income\", data = df, palette = 'viridis')\n",
        "\n",
        "#set label and title\n",
        "plt.title('Annual Income vs Credit Score')\n",
        "plt.xlabel('Credit Score')\n",
        "plt.ylabel('Annual Income')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "A box plot is an excellent choice for visualizing Annual Income vs Credit Score because:\n",
        "* **Relationship between Categorical and Numerical Data:** It effectively displays the distribution of a continuous numerical variable (Annual Income) across different categories of an ordinal variable (Credit Score: 0, 1, 2).\n",
        "* **Shows Key Statistical Measures:** Each box plot clearly indicates the median, interquartile range (IQR), and potential outliers for Annual Income within each Credit Score group.\n",
        "* **Comparison Across Categories:** It allows for a direct visual comparison of income distributions for \"Poor\" (0), \"Standard\" (1), and \"Good\" (2) credit scores, highlighting differences in central tendency and spread."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "* **Positive Correlation:** There's a clear positive relationship: as Credit_Score improves (from 0 to 2), the median Annual Income generally increases.\n",
        "  * **Credit Score 0 (Poor):** Median income is relatively low (around $30,000).\n",
        "  * **Credit Score 1 (Standard):** Median income is higher than for Credit Score 0, but still largely within the mid-range (around $40,000).\n",
        "  * **Credit Score 2 (Good):** Median income is significantly higher than both 0 and 1 (around $45,000 - $50,000).\n",
        "\n",
        "* **Income Spread:** Higher credit scores tend to be associated with a broader range of incomes, particularly for Credit Score 2, which shows a wider interquartile range and reaches higher income levels.\n",
        "\n",
        "* **Outliers:** All credit score categories show numerous outliers at higher annual income levels, suggesting that even individuals with \"Poor\" or \"Standard\" credit scores can have very high incomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "**Positive Business Impact:** Yes, these insights are highly beneficial:\n",
        "\n",
        "* **Refined Credit Scoring Model:** Income is a strong predictor of credit score. This relationship confirms its importance for your predictive model.\n",
        "  * **Reason:** The model will leverage this strong correlation to more accurately classify credit scores, leading to better lending decisions and reduced risk for Paisabazaar.\n",
        "\n",
        "* **Targeted Product/Service Offerings:** Paisabazaar can tailor financial products based on income level and associated credit score.\n",
        "  * **Reason:** For example, higher-income individuals with \"Good\" scores can be offered premium products, while lower-income individuals might be offered specific credit-building solutions, optimizing customer acquisition and engagement.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **\"High Income, Low Score\" Outliers:** The presence of high-income outliers with \"Poor\" or \"Standard\" credit scores (Category 0 and 1) could be a concern.\n",
        "  * **Reason:** These individuals, despite high income, might have other factors (e.g., poor payment history, high debt) impacting their score. Ignoring these complexities could lead to misjudging risk or missing opportunities for proactive financial guidance, potentially leading to defaults or suboptimal customer engagement.\n",
        "\n",
        "* **Excluding Lower Income Potential:** If Paisabazaar solely focuses on high-income segments for \"Good\" scores, they might overlook profitable opportunities within the vast majority of lower-to-mid income customers who still maintain \"Standard\" credit.\n",
        "  * **Reason:** An over-emphasis on high-income segments could limit market reach and overall customer growth if a significant portion of their business historically comes from diverse income brackets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code\n",
        "# @title Chart 7: ID VS Freq (Distribution 1)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create a sample DataFrame (replace with your actual data if needed)\n",
        "_df_0 = pd.DataFrame({\n",
        "    'ID': [101, 102, 103, 104, 105, 102, 104, 103, 102, 101, 104, 104, 105, 105, 106,\n",
        "           107, 108, 109, 110, 102, 101, 103, 104, 106, 105, 105, 108, 109, 110, 110]\n",
        "})\n",
        "\n",
        "# Step 2: Plot a histogram of the 'ID' column\n",
        "_df_0['ID'].plot(kind='hist', bins=20, title='ID Distribution', color='skyblue', edgecolor='black')\n",
        "\n",
        "# Step 3: Style the plot\n",
        "plt.xlabel('ID')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "\n",
        "# Step 4: Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "* **Unique Identifier:** The ID column serves as a unique identifier for each record. By its nature, each ID should be distinct.\n",
        "\n",
        "* **Meaningless Distribution:** Visualizing the frequency distribution of a unique identifier doesn't provide meaningful insights into the dataset's characteristics or relationships between variables. Its frequency should ideally be 1 for each unique ID, indicating no duplicates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "* **Each ID is Unique:** The chart shows a frequency of 1.0 for each displayed ID (e.g., 5634.0, 5635.0, etc.). This confirms that each ID value is unique within the sampled range shown in the plot.\n",
        "\n",
        "* **Sequential or Discrete Nature**: The IDs appear to be numerical and possibly sequential or at least discrete, given the distinct bars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "**Positive Business Impact:** No direct positive business impact from this specific chart, as it represents a data quality check rather than an analytical insight.\n",
        " * **Data Integrity Confirmation:** It confirms that the ID column correctly serves its purpose as a unique identifier (i.e., no duplicate IDs in the displayed sample).\n",
        "    * **Reason:** Maintaining unique identifiers is fundamental for data integrity, accurate record-keeping, and database operations.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **No Analytical Value:** Relying on this chart for business decisions or predictive modeling would be detrimental.\n",
        "  * **Reason:** An ID itself has no predictive power for credit scores or other business metrics. Including it as a feature in a model would lead to overfitting and poor generalization, impacting the model's reliability and hindering positive business outcomes. This is why we dropped it in the data preparation phase.\n",
        "\n",
        "* **Misinterpretation Risk:** Misinterpreting this chart as providing meaningful patterns could lead to misguided strategies.\n",
        "  * **Reason:** For instance, concluding that a specific ID range is more common is irrelevant for business strategy if IDs are simply assigned sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample DataFrame with 'Month' column\n",
        "# Replace this with your actual data source if needed\n",
        "_df_1 = pd.DataFrame({\n",
        "    'Month': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] * 10  # 10 years of months\n",
        "})\n",
        "\n",
        "# Plot histogram of the 'Month' column\n",
        "plt.figure(figsize=(8, 5))\n",
        "_df_1['Month'].plot(kind='hist', bins=20, title='Month Distribution', color='skyblue', edgecolor='black')\n",
        "\n",
        "# Clean up the plot aesthetics\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "* **Discrete Numerical Variable:** Month is a discrete numerical variable (likely representing months 1 through 12). A bar chart would be more appropriate to show counts per month if they were categorical. As a histogram, it shows frequency within numerical bins.\n",
        "\n",
        "* **Uniformity Indication:** This specific plot suggests a uniform distribution across the months shown, implying a similar number of records per month in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "* **Uniform Distribution (for months 1-5):** The chart shows that for the months displayed (1 through 5), each month has an approximately equal frequency (a value of 1.0 on the y-axis, implying relative proportion if normalized, or absolute count if frequency is scaled).\n",
        "\n",
        "* **Data Collection Over Time:** This indicates that data collection or customer interactions appear to be evenly distributed across these initial months, at least for the sample shown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "**Positive Business Impact:** No direct, immediate positive business impact from this chart's insights.\n",
        "\n",
        "* **Data Consistency:** It suggests that data collection is consistent across time for these months.\n",
        "  * **Reason:** This implies reliable and unbiased data capture over the observed period, which is crucial for building robust predictive models.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **Lack of Seasonal Insight:** If the dataset spans a full year, seeing only uniform distribution for months 1-5 means missing potential seasonality or trends across the full 12 months.\n",
        "  * **Reason:** If certain months have higher loan applications, defaults, or credit card usage, not identifying these patterns could lead to suboptimal staffing, marketing, or risk management strategies, hindering growth.\n",
        "\n",
        "* **Limited Predictive Value:** Month as a raw numerical feature might not have direct predictive power if relationships aren't truly linear or cyclical.\n",
        "  * **Reason:** Treating it simply as a number without considering its cyclical nature (e.g., as a categorical feature, or using sine/cosine transformations for seasonality) could limit the model's ability to capture time-based effects, reducing its accuracy and thus negatively impacting business decisions based on its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart - 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample DataFrame creation (replace this with your actual data loading method)\n",
        "# For example: _df_2 = pd.read_csv(\"your_file.csv\")\n",
        "_df_2 = pd.DataFrame({\n",
        "    'Delay_from_due_date': [0, 1, -2, 5, 3, -1, 0, 2, 4, -3, 1, 1, 2, 3, 0, -2, 4, 5, 3, -1]\n",
        "})\n",
        "\n",
        "# Chart - 9: Delay from due date VS Frequency (Histogram)\n",
        "_df_2['Delay_from_due_date'].plot(kind='hist', bins=20, title='Delay_from_due_date')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.xlabel('Days Delayed')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "A histogram is used for Delay_from_due_date to show the frequency distribution of the numerical delay values. While it could be treated as discrete, a histogram effectively bins the data to show where most delays occur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "* **Most Common Delay:** The most frequent delay observed is 3 days, with a frequency of 3.0 (likely representing thousands of instances given the y-axis scale).\n",
        "* **Other Observed Delays:** Delays of 5 days and 6 days are also present, but with significantly lower frequencies (1.0 each).\n",
        "* **Narrow Range:** The chart indicates that observed delays are concentrated in a very narrow range, primarily 3, 5, and 6 days, with 3 days being the most common. There are no shown delays between these values or at higher/lower extremes within the displayed range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "**Positive Business Impact:** Yes, these insights are highly beneficial:\n",
        "\n",
        "* **Early Intervention & Collections Strategy:** Knowing the most common delay (3 days) allows Paisabazaar to implement proactive and timely intervention strategies.\n",
        "  * **Reason:** This enables setting up automated reminders or initial contact efforts at the 3-day mark to encourage payment, potentially preventing longer delays and reducing the cost of collections and loan defaults.\n",
        "* **Refined Risk Assessment:** The specific delay patterns can be critical features in the credit score prediction model.\n",
        "  * **Reason:** Models can learn that consistent 3-day delays might indicate a different risk profile than longer, less frequent delays, leading to more accurate credit assessments.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **Ignoring Longer/More Frequent Delays:** If the dataset predominantly shows short, specific delays, it might lead to underestimating the risk posed by customers who experience longer or more frequent delays not visible in this snapshot.\n",
        "  * **Reason:** Focusing only on the most common short delays might cause Paisabazaar to overlook the early signs of severe financial distress, leading to unmitigated defaults and negative growth.\n",
        "\n",
        "* **Ineffective Communication Timing:** If customer communication strategies are not aligned with these common delay patterns (e.g., waiting too long to contact after the due date), it could be less effective.\n",
        "  * **Reason:** Delayed communication increases the likelihood of a loan becoming delinquent, impacting revenue and increasing recovery costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      },
      "source": [
        "#### Chart - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "outputs": [],
      "source": [
        "# Chart - 10 visualization code\n",
        "# @title Chart 10: Num of Delayed Payement VS Freq (Distribution 4)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample creation of _df_3 (you should replace this with actual data loading)\n",
        "# Example:\n",
        "# _df_3 = pd.read_csv(\"your_data_file.csv\")\n",
        "\n",
        "# For demonstration, here's a sample DataFrame:\n",
        "_df_3 = pd.DataFrame({\n",
        "    'Num_of_Delayed_Payment': [0, 1, 2, 2, 3, 5, 5, 5, 7, 9, 10, 10, 11, 13, 15, 18, 20, 25]\n",
        "})\n",
        "\n",
        "# Plotting the histogram\n",
        "_df_3['Num_of_Delayed_Payment'].plot(kind='hist', bins=20, title='Num_of_Delayed_Payment')\n",
        "plt.xlabel('Number of Delayed Payments')\n",
        "plt.ylabel('Frequency')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "A histogram is used for Num_of_Delayed_Payment to show the frequency distribution of discrete numerical counts. It effectively highlights how many times customers typically delay payments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "* **Bimodal Distribution:** The chart shows two primary peaks, indicating two common scenarios for the number of delayed payments.\n",
        "* **Most Common Delay Counts:** The highest frequency is for 4 delayed payments (frequency of 3.0), followed by 7 delayed payments (frequency of 2.0).\n",
        "* **Absence of Other Counts:** The plot suggests that there are no or very few customers with 5 or 6 delayed payments within the displayed range, highlighting distinct clusters of delay behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "**Positive Business Impact:** Yes, these insights are highly beneficial:\n",
        "\n",
        "* **Behavioral Segmentation & Risk Profiling:** Identifying distinct groups with 4 and 7 delayed payments allows Paisabazaar to segment customers based on their payment behavior.\n",
        "  * **Reason:** This enables creating targeted risk profiles and developing specific strategies (e.g., early warning systems for those approaching 7 delays, or offering financial education to mitigate future delays), leading to better risk management and reduced defaults.\n",
        "\n",
        "**Tailored Collections Strategies:** Different strategies can be deployed for customers with 4 vs. 7 delayed payments.\n",
        "  * **Reason:** More aggressive or personalized collections efforts might be needed for the 7-delay group, while the 4-delay group might benefit from gentler reminders, optimizing resource allocation and recovery rates.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **Ignoring the High-Risk Group:** The presence of a significant group with 7 delayed payments represents a considerable risk.\n",
        "  * **Reason:** If this group isn't identified and managed with appropriate collection or support measures, it will lead to higher non-performing assets and direct financial losses, negatively impacting growth.\n",
        "\n",
        "* **Binary View of Delays:** If the data only shows 4 or 7 delays and other counts are truly absent, it might oversimplify payment behavior.\n",
        "  * **Reason:** Assuming customers only fall into these two buckets without understanding the context of why no one has 5 or 6 delays (e.g., data artifact vs. actual behavior) could lead to an incomplete risk assessment, potentially missing subtle signs of distress and impacting accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### Chart - 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Example definition of _df_4 (replace with your actual dataset)\n",
        "# For example, loading from CSV:\n",
        "# _df_4 = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Sample data (remove if you're using your own file)\n",
        "_df_4 = pd.DataFrame({\n",
        "    'Payment_Behaviour': ['Regular', 'Regular', 'Delinquent', 'Irregular', 'Regular', 'Delinquent', 'Irregular', 'Delinquent']\n",
        "})\n",
        "\n",
        "# Chart 11: Payment Behaviour Graph (Categorical Distribution)\n",
        "_df_4.groupby('Payment_Behaviour').size().plot(\n",
        "    kind='barh',\n",
        "    color=sns.color_palette('Dark2')\n",
        ")\n",
        "plt.title(\"Payment Behaviour Distribution\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "A  horizontal bar chart is appropriate for Payment_Behaviour because:\n",
        "\n",
        "* **Categorical Variable:** Payment_Behaviour is a nominal categorical variable with distinct categories.\n",
        "\n",
        "* **Distribution of Categories:** It effectively displays the frequency or proportion of each payment behavior category within the dataset.\n",
        "\n",
        "* **Readability:** For categories with long labels (like \"Low_spent_Small_value_payments\"), horizontal bars improve readability compared to vertical bars where labels might overlap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "* **Uniform Distribution:** All Payment_Behaviour categories show an approximately equal distribution, with each having a frequency close to 1.0. This indicates that customers are almost evenly distributed across these different spending and payment value behaviors.\n",
        "\n",
        "* **Balanced Data for Modeling:** This uniformity suggests that there isn't a significant class imbalance within the Payment_Behaviour feature, which is generally good for machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "**Positive Business Impact:** Yes, these insights are beneficial:\n",
        "\n",
        "* **Robust Customer Segmentation:** The balanced distribution of payment behaviors enables Paisabazaar to create distinct and meaningful customer segments based on spending habits and payment value.\n",
        "  * **Reason:** This allows for highly targeted product development, marketing campaigns, and customer service strategies tailored to each behavior type, leading to higher engagement and satisfaction.\n",
        "\n",
        "* **Enhanced Model Performance:** A relatively even distribution of a key categorical feature like Payment_Behaviour is ideal for machine learning models.\n",
        "  * **Reason:** It ensures that the model has sufficient examples from each behavior type to learn patterns effectively, potentially leading to more accurate predictions for creditworthiness across the spectrum of customer behaviors.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **Lack of Prioritization without Further Analysis:** While balanced is good for modeling, it doesn't immediately highlight which payment behaviors are most \"desirable\" or \"undesirable\" from a business perspective (e.g., which lead to higher profit or higher risk).\n",
        "  * **Reason:** Without further analysis (e.g., cross-tabulating Payment_Behaviour with Credit_Score or default rates), Paisabazaar might treat all segments equally. This could lead to a missed opportunity to prioritize efforts on behaviors that correlate with better credit outcomes or to proactively manage those correlated with higher risk, thus potentially hindering optimal business growth.\n",
        "\n",
        "* **Static View:** The chart provides a static snapshot of current behavior.\n",
        "  * **Reason:** If payment behaviors change over time, relying solely on this distribution without monitoring trends could lead to outdated strategies, potentially impacting customer retention or risk management negatively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "#### Chart - 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example DataFrame for demonstration\n",
        "_df_5 = pd.DataFrame({\n",
        "    'ID': range(1, 101),\n",
        "    'Month': [i % 12 + 1 for i in range(1, 101)]  # Cycles through months 1 to 12\n",
        "})\n",
        "\n",
        "# Chart - 12 visualization code\n",
        "# Chart 12: ID VS Month (2D Distribution - 1)\n",
        "_df_5.plot(kind='scatter', x='ID', y='Month', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.title('Chart 12: ID VS Month')\n",
        "plt.xlabel('ID')\n",
        "plt.ylabel('Month')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "* **Relationship between Two Numerical Variables:** A scatter plot is typically used to visualize the relationship or correlation between two numerical variables.\n",
        "* **Sequential Nature:** Given ID is a unique identifier (likely sequential) and Month represents a sequential period, the plot inherently shows a progression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "* **Sequential ID-Month Mapping:** The chart shows a direct, linear relationship: as ID increases, Month also increases.\n",
        "* **Data Collection Progression: **This suggests that records are likely being entered sequentially, with newer IDs corresponding to later months. Each specific ID in the plot corresponds to a specific month (e.g., ID 5634.0 is Month 1, ID 5638.0 is Month 5).\n",
        "* **No Duplicates in Sample:** The distinct points indicate that each ID-Month pair in the sample is unique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "**Positive Business Impact:** No direct positive business impact from this chart for predictive modeling, but it offers data quality assurance:\n",
        "* **Data Integrity/Audit Trail:** Confirms data is being recorded chronologically with unique IDs.\n",
        "  * **Reason:** This ensures good data hygiene, which is foundational for reliable analysis and operational processes.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **No Predictive Value:** The plot offers no analytical insight for predicting credit scores or business outcomes.\n",
        "  * **Reason:** Including ID or a direct derivation from it as a feature would lead to severe overfitting, as a model would memorize IDs rather than learn general patterns, resulting in poor performance on new data and hindering business growth.\n",
        "\n",
        "* **Misleading Simplicity:** The seemingly \"clean\" linear relationship might falsely suggest analytical value.\n",
        "  * **Reason:** Over-reliance on such plots for non-analytical variables can distract from more impactful insights, leading to wasted effort and potentially flawed strategic decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9LCva-p1cl"
      },
      "source": [
        "#### Chart - 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "outputs": [],
      "source": [
        "# Chart - 13 visualization code\n",
        "# @title Chart 13: Month VS Delay from due date (2D Distribution - 2)\n",
        "_df_6 = df[['Month', 'Delay_from_due_date']].copy()\n",
        "_df_6.plot(kind='scatter', x='Month', y='Delay_from_due_date', s=32, alpha=0.8)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.title(\"Month vs Delay from Due Date\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Delay from Due Date\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MkPsBcp1cl"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22bRsFWp1cl"
      },
      "source": [
        "A scatter plot is used for Month vs Delay_from_due_date to explore the relationship between two numerical variables. It can reveal if there's any trend or pattern in payment delays as the months progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cELzS2fp1cl"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      },
      "source": [
        "* **Increasing Delay Trend:** The plot suggests a general increasing trend in Delay_from_due_date as Month progresses.\n",
        "  * Months 1, 2, and 3 show a consistent 3-day delay.\n",
        "  * Month 4 shows a 5-day delay.\n",
        "  * Month 5 shows a 6-day delay.\n",
        "\n",
        "* **Specific Delay Points:** Delays appear to occur at specific, discrete values (3, 5, 6 days) rather than a continuous range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPXvC8up1cl"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL8l1tdLp1cl"
      },
      "source": [
        "**Positive Business Impact:** Yes, these insights are valuable:\n",
        "\n",
        "* **Proactive Risk Management & Intervention Timing:** Identifying that delays tend to increase in later months (e.g., from month 4 onwards) allows Paisabazaar to anticipate and proactively increase monitoring or send earlier reminders in those months.\n",
        "  * **Reason:** This can help prevent longer payment delays and reduce the number of non-performing loans, directly improving financial health.\n",
        "* **Refined Predictive Modeling:** This time-dependent pattern of delays is a crucial feature for the credit score model.\n",
        "  * **Reason:** Incorporating this trend will allow the model to better predict changes in credit behavior over time, leading to more accurate credit risk assessments.\n",
        "\n",
        "**Insights that Lead to Negative Growth:**\n",
        "\n",
        "* **Escalating Delinquency if Unchecked:** If this increasing trend of delays over time is not addressed through interventions, it could signify a worsening customer payment behavior.\n",
        "  * **Reason:** A consistent increase in Delay_from_due_date over consecutive months can lead to higher rates of loan delinquency and eventual defaults, directly impacting Paisabazaar's revenue and increasing recovery costs.\n",
        "\n",
        "* **Seasonal/Temporal Blind Spots:** This plot only covers months 1-5. Assuming this trend continues or applies to all months without data for the full year could be misleading.\n",
        "  * **Reason:** Missing larger seasonal patterns could lead to ineffective strategies during other periods, potentially causing unexpected spikes in defaults or missed revenue opportunities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### chart-14 Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# @title Correlation Heatmap\n",
        "print(\"\\n--- Generating Correlation Heatmap ---\")\n",
        "plt.figure(figsize=(20, 18)) # Adjust figure size for better readability\n",
        "# Select only numerical columns for correlation calculation\n",
        "numerical_df = df.select_dtypes(include=[np.number])\n",
        "correlation_matrix = numerical_df.corr()\n",
        "sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=.5) # annot=False due to many columns\n",
        "plt.title('Correlation Heatmap of All Numerical Features', fontsize=20)\n",
        "plt.show()\n",
        "print(\"Correlation Heatmap generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "A correlation heatmap is an excellent choice for visualizing the relationships between numerical features because:\n",
        "* **Identifies Relationships:** It visually represents the pairwise correlation coefficients between all numerical variables in the dataset, indicating the strength and direction (positive or negative) of linear relationships.\n",
        "* **Feature Selection/Engineering:** It's crucial for identifying highly correlated features (which might indicate multicollinearity, an issue for some models) and for understanding which features might be strong predictors of the target variable (Credit Score).\n",
        "* **Comprehensive Overview:** For a dataset with many numerical features (like yours, after encoding), a heatmap provides a quick and comprehensive overview of all bivariate relationships in one glance.\n",
        "* **Color-Coded Strength:** The use of a color gradient (e.g., coolwarm) makes it easy to spot strong positive (red) and strong negative (blue) correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "* **Strong Positive Correlation with Credit Score:**\n",
        "  * Annual_Income and Monthly_Inhand_Salary show a strong positive correlation with Credit_Score (indicated by reddish squares). This confirms that higher income generally leads to better credit scores.\n",
        "  * Monthly_Balance also shows a positive correlation with Credit_Score.\n",
        "\n",
        "* **Strong Negative Correlation with Credit Score:**\n",
        "  * Outstanding_Debt, Credit_Utilization_Ratio, Num_of_Loan, Delay_from_due_date, and Num_of_Delayed_Payment show negative correlations with Credit_Score (bluish squares). This means higher debt, utilization, and payment delays are associated with lower credit scores.\n",
        "\n",
        "* **High Multicollinearity among Income/Salary Features:**\n",
        "  * Annual_Income and Monthly_Inhand_Salary are very highly positively correlated with each other (dark red). This is expected as monthly in-hand salary is derived from annual income.\n",
        "  * Outstanding_Debt and Credit_Utilization_Ratio are also strongly positively correlated.\n",
        "\n",
        "* **High Multicollinearity among Loan Types and Occupations:**\n",
        "  * Many of the one-hot encoded Loan_ and Occupation_ columns show low to moderate correlations among themselves and with other main numerical features, but some specific pairs might have higher correlations (e.g., a customer having one loan type might often have another).\n",
        "\n",
        "* **Credit Mix and Payment of Min Amount:**\n",
        "  * Credit_Mix (which was encoded as ordinal) and Payment_of_Min_Amount show moderate positive correlations with Credit_Score, indicating their importance.\n",
        "\n",
        "* **Credit_History_Age Issue Confirmed:** The column Credit_History_Age shows almost no correlation with any other feature, appearing as mostly white/light-colored rows/columns. This visually confirms the earlier observation from df.info() that this column might be empty or have issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### chart-15 Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "# @title Pair Plot\n",
        "print(\"\\n--- Generating Pair Plot for Key Features ---\")\n",
        "# Select a subset of numerical columns for the pair plot for better readability\n",
        "# Include the target variable 'Credit_Score' to see relationships with it\n",
        "key_features = [\n",
        "    'Annual_Income',\n",
        "    'Monthly_Inhand_Salary',\n",
        "    'Outstanding_Debt',\n",
        "    'Credit_Utilization_Ratio',\n",
        "    'Total_EMI_per_month',\n",
        "    'Monthly_Balance',\n",
        "    'Age',\n",
        "    'Num_of_Loan',\n",
        "    'Interest_Rate',\n",
        "    'Credit_Score' # Target variable\n",
        "]\n",
        "\n",
        "# Ensure all key_features exist in the DataFrame after preprocessing\n",
        "available_key_features = [col for col in key_features if col in df.columns]\n",
        "if len(available_key_features) < len(key_features):\n",
        "    print(f\"Warning: Some key features selected for pair plot are missing from DataFrame: {list(set(key_features) - set(available_key_features))}\")\n",
        "\n",
        "# Use the available key features\n",
        "if available_key_features:\n",
        "    # Convert Credit_Score back to categorical labels for better visualization in pairplot hue\n",
        "    temp_df_for_plot = df[available_key_features].copy()\n",
        "    credit_score_labels = {0: 'Poor', 1: 'Standard', 2: 'Good'}\n",
        "    temp_df_for_plot['Credit_Score_Category'] = temp_df_for_plot['Credit_Score'].map(credit_score_labels)\n",
        "\n",
        "    # Use hue for Credit_Score_Category if it's not all one value\n",
        "    if temp_df_for_plot['Credit_Score_Category'].nunique() > 1:\n",
        "        sns.pairplot(temp_df_for_plot, hue='Credit_Score_Category', diag_kind='kde', markers=[\"o\", \"s\", \"D\"])\n",
        "    else:\n",
        "        sns.pairplot(temp_df_for_plot, diag_kind='kde') # If only one category, no hue needed\n",
        "\n",
        "    plt.suptitle('Pair Plot of Key Numerical Features colored by Credit Score', y=1.02, fontsize=16) # Adjust title position\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No key features available for pair plot after preprocessing.\")\n",
        "\n",
        "print(\"Pair Plot generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "A pair plot is chosen for multivariate analysis of a subset of numerical features and the target variable (Credit_Score) because:\n",
        "\n",
        "* **Bivariate Relationships:** It simultaneously displays the pairwise relationships (scatter plots) between all selected variables.\n",
        "* **Univariate Distributions:** The diagonal shows the distribution of each individual variable (histograms/KDEs), often split by the hue variable (Credit_Score).\n",
        "* **Target Variable Insights:** By coloring points by Credit_Score, it reveals how different credit score categories cluster or separate across various feature combinations, which is crucial for understanding feature importance and building a predictive model.\n",
        "* **Initial Feature Engineering Guidance:** It can suggest non-linear relationships or interaction effects that might be useful for further feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "Across both attached images, focusing on the relationships colored by Credit_Score (Poor=Orange, Standard=Blue, Good=Green):\n",
        "\n",
        "* **Annual_Income and Monthly_Inhand_Salary vs. Credit Score:**\n",
        "  * As observed in the box plot, higher Annual_Income and Monthly_Inhand_Salary generally correlate with better Credit_Score (more green/blue points at higher income levels).\n",
        "  * There's still a significant overlap, especially between \"Poor\" and \"Standard\" scores at lower to mid-income levels.\n",
        "  * The strong linear relationship between Annual_Income and Monthly_Inhand_Salary is visible in their scatter plot (top left).\n",
        "\n",
        "* **Outstanding_Debt and Credit_Utilization_Ratio vs. Credit Score:**\n",
        "  * Higher Outstanding_Debt and Credit_Utilization_Ratio tend to be associated with \"Poor\" (orange) credit scores.\n",
        "  * \"Good\" (green) credit scores are predominantly found at lower debt and utilization levels.\n",
        "  * The diagonal distribution of Credit_Utilization_Ratio shows the bell shape we discussed, with \"Good\" scores mostly on the left tail and \"Poor\" on the right.\n",
        "\n",
        "* **Total_EMI_per_month and Monthly_Balance vs. Credit Score:**\n",
        "  * Lower Total_EMI_per_month and higher Monthly_Balance are more associated with \"Good\" credit scores.\n",
        "  * \"Poor\" scores tend to have higher EMIs and lower monthly balances.\n",
        "\n",
        "* **Age vs. Credit Score:**\n",
        "  * \"Good\" credit scores appear more frequently in slightly older age brackets (mid-30s to 40s) compared to \"Poor\" scores which are more spread out, including younger ages.\n",
        "  * The overall age distribution confirms previous observations, but now we see how credit score groups populate these age bins.\n",
        "\n",
        "* **Num_of_Loan vs. Credit Score:**\n",
        "  * \"Good\" credit scores are typically associated with a moderate number of loans (e.g., 2-4), while \"Poor\" scores can be found across various loan counts, including very high numbers.\n",
        "  * Very high Num_of_Loan values are almost exclusively orange (\"Poor\").\n",
        "\n",
        "* **Interest_Rate vs. Credit Score:**\n",
        "  * Lower Interest_Rates are strongly associated with \"Good\" credit scores (green points clustered at lower interest rates).\n",
        "  * Higher Interest_Rates are almost exclusively associated with \"Poor\" credit scores (orange points at higher rates). This is a very strong relationship.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "**Null Hypothesis (H0):** There is no significant difference in the mean Annual Income across the different Credit Score categories (Poor, Standard, Good).\n",
        "\n",
        "**Alternate Hypothesis (H1):** There is a significant difference in the mean Annual Income for at least one pair of Credit Score categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Extract Annual Income for each Credit Score category\n",
        "income_poor = df[df['Credit_Score'] == 0]['Annual_Income']\n",
        "income_standard = df[df['Credit_Score'] == 1]['Annual_Income']\n",
        "income_good = df[df['Credit_Score'] == 2]['Annual_Income']\n",
        "\n",
        "# Perform one-way ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(income_poor, income_standard, income_good)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4e}\") # Using scientific notation for small p-values\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"\\nResult: Reject the null hypothesis.\")\n",
        "    print(\"Conclusion: There is a statistically significant difference in mean Annual Income across Credit Score categories.\")\n",
        "else:\n",
        "    print(\"\\nResult: Fail to reject the null hypothesis.\")\n",
        "    print(\"Conclusion: There is no statistically significant difference in mean Annual Income across Credit Score categories.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "One-Way Analysis of Variance (ANOVA) test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "**ANOVA:** because it is suitable for comparing the means of a continuous variable (Annual_Income) across three or more independent groups (the three Credit_Score categories: Poor, Standard, Good). This test helps determine if the observed differences in mean income between these credit score groups are statistically significant or due to random chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "**Null Hypothesis (H0):** There is no significant difference in the mean Credit Utilization Ratio across the different Credit Score categories (Poor, Standard, Good).\n",
        "\n",
        "**Alternate Hypothesis (H1):** There is a significant difference in the mean Credit Utilization Ratio for at least one pair of Credit Score categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Extract Credit Utilization Ratio for each Credit Score category\n",
        "utilization_poor = df[df['Credit_Score'] == 0]['Credit_Utilization_Ratio']\n",
        "utilization_standard = df[df['Credit_Score'] == 1]['Credit_Utilization_Ratio']\n",
        "utilization_good = df[df['Credit_Score'] == 2]['Credit_Utilization_Ratio']\n",
        "\n",
        "# Perform one-way ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(utilization_poor, utilization_standard, utilization_good)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4e}\") # Using scientific notation for small p-values\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"\\nResult: Reject the null hypothesis.\")\n",
        "    print(\"Conclusion: There is a statistically significant difference in mean Credit Utilization Ratio across Credit Score categories.\")\n",
        "else:\n",
        "    print(\"\\nResult: Fail to reject the null hypothesis.\")\n",
        "    print(\"Conclusion: There is no statistically significant difference in mean Credit Utilization Ratio across Credit Score categories.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "One-Way Analysis of Variance (ANOVA) test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "ANOVA was chosen to compare the mean Credit_Utilization_Ratio values across the three Credit_Score groups. This helps determine if credit utilization significantly varies based on the credit score, which was visually suggested by the pair plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "**Null Hypothesis (H0):** There is no significant difference in the mean Monthly Balance across the different Credit Score categories (Poor, Standard, Good).\n",
        "\n",
        "**Alternate Hypothesis (H1):** There is a significant difference in the mean Monthly Balance for at least one pair of Credit Score categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Extract Monthly Balance for each Credit Score category\n",
        "balance_poor = df[df['Credit_Score'] == 0]['Monthly_Balance']\n",
        "balance_standard = df[df['Credit_Score'] == 1]['Monthly_Balance']\n",
        "balance_good = df[df['Credit_Score'] == 2]['Monthly_Balance']\n",
        "\n",
        "# Perform one-way ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(balance_poor, balance_standard, balance_good)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4e}\") # Using scientific notation for small p-values\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"\\nResult: Reject the null hypothesis.\")\n",
        "    print(\"Conclusion: There is a statistically significant difference in mean Monthly Balance across Credit Score categories.\")\n",
        "else:\n",
        "    print(\"\\nResult: Fail to reject the null hypothesis.\")\n",
        "    print(\"Conclusion: There is no statistically significant difference in mean Monthly Balance across Credit Score categories.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "One-Way Analysis of Variance (ANOVA) test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "ANOVA was selected to compare the average Monthly_Balance among the three distinct Credit_Score groups. This test is appropriate for assessing if the observed differences in mean monthly balance across these credit score classifications are statistically meaningful, aligning with visual patterns from the pair plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "print(\"Handling Missing Values & Missing Value Imputation\")\n",
        "print(\"Checking for missing values after initial preprocessing:\")\n",
        "\n",
        "missing_values_before_imputation = df.isnull().sum()\n",
        "print(missing_values_before_imputation[missing_values_before_imputation > 0])\n",
        "\n",
        "# Impute missing values if any are still present\n",
        "if missing_values_before_imputation.sum() > 0:\n",
        "    print(\"\\nProceeding with missing value imputation for remaining columns.\")\n",
        "\n",
        "    # Impute numerical columns with median (robust to outliers)\n",
        "    for col in df.select_dtypes(include=np.number).columns:\n",
        "        if df[col].isnull().any():\n",
        "            median_val = df[col].median()\n",
        "            df[col].fillna(median_val, inplace=True)\n",
        "            print(f\"Filled missing values in '{col}' with median: {median_val}\")\n",
        "\n",
        "    # Impute categorical columns (shouldn't be any left after initial preprocessing) with mode\n",
        "    for col in df.select_dtypes(include='object').columns:\n",
        "        if df[col].isnull().any():\n",
        "            mode_val = df[col].mode()[0]\n",
        "            df[col].fillna(mode_val, inplace=True)\n",
        "            print(f\"Filled missing values in '{col}' with mode: {mode_val}\")\n",
        "\n",
        "    print(\"\\nMissing value handling complete. Re-checking missing values:\")\n",
        "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
        "\n",
        "else:\n",
        "    print(\"No missing values found in the DataFrame. All values are non-null.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "1. **Median Imputation for Numerical Columns:** For all columns identified as numerical (e.g., float64, int64), any missing values (NaN) are filled with the median value of that specific column.\n",
        "  * **Robustness to Outliers:** The median is a robust measure of central tendency, meaning it is less affected by extreme values or outliers in the data compared to the mean. If a numerical column contains outliers, using the mean for imputation could distort the distribution and introduce bias. Using the median helps to preserve the original distribution more accurately.\n",
        "  * **Preserves Data Integrity:** It fills in missing data using an actual value from the dataset's central tendency, providing a reasonable estimate without creating new, artificial values far outside the typical range.\n",
        "\n",
        "2. **Mode Imputation for Categorical Columns:** For any columns identified as object dtype (which typically represent categorical or text data), any missing values are filled with the mode (the most frequently occurring value) of that column.\n",
        "  * **Applicability:** For categorical data, statistical measures like mean or median are not meaningful. The mode is the most appropriate measure of central tendency for nominal or ordinal categorical variables.\n",
        "  * **Maintains Distribution:** Imputing with the mode helps to maintain the original distribution of categories within the column, as it replaces missing values with the most common existing category.\n",
        "\n",
        "**Additional Context in the Code:**\n",
        "It's also important to note that the code first checks if any missing values exist at all (df.isnull().sum().sum() > 0). If no missing values are detected, the imputation steps are skipped, indicating that the dataset is already clean from a missing values perspective. This was largely the case after the initial data preparation you requested, where most \"object\" columns were transformed or dropped, leading to a largely complete numerical dataset. The imputation section acts as a safeguard for any remaining or newly introduced NaNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "print(\"\\nHandling Outliers & Outlier Treatments (Capping using IQR)\")\n",
        "\n",
        "# Identify numerical columns for outlier treatment (exclude binary/encoded columns like Loan_, Occupation_, Payment_Behaviour_)\n",
        "# We also exclude 'Month' as it's a discrete categorical feature and 'Credit_Score' as it's the target.\n",
        "numerical_cols_for_outliers = [\n",
        "    'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts',\n",
        "    'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan',\n",
        "    'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit',\n",
        "    'Num_Credit_Inquiries', 'Outstanding_Debt', 'Credit_Utilization_Ratio',\n",
        "    'Credit_History_Age', 'Total_EMI_per_month', 'Amount_invested_monthly',\n",
        "    'Monthly_Balance', 'Age'\n",
        "]\n",
        "\n",
        "# Filter for columns that actually exist in the DataFrame\n",
        "numerical_cols_for_outliers = [col for col in numerical_cols_for_outliers if col in df.columns]\n",
        "\n",
        "if not numerical_cols_for_outliers:\n",
        "    print(\"No suitable numerical columns found for outlier treatment.\")\n",
        "else:\n",
        "    for col in numerical_cols_for_outliers:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "\n",
        "        # Count outliers before capping\n",
        "        outliers_upper = df[df[col] > upper_bound].shape[0]\n",
        "        outliers_lower = df[df[col] < lower_bound].shape[0]\n",
        "\n",
        "        if outliers_upper > 0 or outliers_lower > 0:\n",
        "            df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
        "            df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
        "            print(f\"Outliers in '{col}' capped at [{lower_bound:.2f}, {upper_bound:.2f}]. Upper outliers treated: {outliers_upper}, Lower outliers treated: {outliers_lower}\")\n",
        "        else:\n",
        "            print(f\"No significant outliers found in '{col}' based on IQR method.\")\n",
        "\n",
        "print(\"Outlier treatment complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "**IQR (Interquartile Range) Based Capping (Winsorization):** For identified numerical columns, outliers are treated by capping them. This means any values below a calculated lower bound are replaced with the lower bound, and any values above a calculated upper bound are replaced with the upper bound.\n",
        "\n",
        "* **Calculation:**\n",
        "  * The First Quartile (Q1) is calculated (25th percentile).\n",
        "  * The Third Quartile (Q3) is calculated (75th percentile).\n",
        "  * The Interquartile Range (IQR) is calculated as IQR=Q3Q1.\n",
        "  * The Upper Bound for outliers is defined as UpperBound=Q3+1.5IQR.\n",
        "  * The Lower Bound for outliers is defined as LowerBound=Q11.5IQR.\n",
        "\n",
        "* **Robustness to Extreme Values:** The IQR method is robust to extreme values because it relies on quartiles rather than the mean and standard deviation, which are heavily influenced by outliers. This makes it particularly suitable for skewed distributions, which are common in real-world datasets (like Annual_Income in your data).\n",
        "\n",
        "* **Preserves Data Integrity (relative to removal):** Instead of removing outlier rows (which can lead to data loss and reduced sample size), capping preserves all observations. This is important for maintaining the overall structure and size of the dataset, especially if outliers represent valid, albeit extreme, data points.\n",
        "\n",
        "* **Common and Interpretable:** It's a widely accepted and intuitive method for outlier treatment that effectively limits the influence of extreme values without drastically altering the rest of the data.\n",
        "Simple Implementation: It's relatively straightforward to implement and interpret compared to more complex methods.\n",
        "\n",
        "**Columns Selected for Outlier Treatment:**\n",
        "\n",
        "The code specifically targets key numerical features that are prone to outliers in real-world scenarios, such as Annual_Income, Monthly_Inhand_Salary, Outstanding_Debt, Credit_Utilization_Ratio, Age, etc. It explicitly excludes binary/encoded columns and the Credit_Score target variable, as outlier treatment is not appropriate for these types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "print(\"\\nCategorical Column Encoding Status\")\n",
        "\n",
        "# Check for remaining object (categorical) columns\n",
        "remaining_object_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "if not remaining_object_cols:\n",
        "    print(\"All categorical columns have already been encoded in the 'Data Preparation for Analysis' section.\")\n",
        "    print(\"No further encoding is required for the current DataFrame.\")\n",
        "\n",
        "else:\n",
        "    print(\"The following categorical columns still exist and need encoding:\")\n",
        "    print(remaining_object_cols)\n",
        "    print(\"Please specify the encoding method (e.g., One-Hot Encoding, Label Encoding) if these columns are intended for the model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "1.  **Ordinal Mapping (Manual/Dictionary Mapping):**\n",
        "    * For `Credit_Mix`, `Payment_of_Min_Amount`, and `Credit_Score`, a dictionary-based mapping was applied to convert categorical labels into numerical values. The mapping preserves the inherent order or hierarchy of the categories (e.g., 'Bad' < 'Standard' < 'Good' for `Credit_Mix` and `Credit_Score`).\n",
        "\n",
        "        * **Preserves Order:** These variables have a clear, meaningful order. Ordinal mapping ensures that this order is numerically represented, which can be beneficial for machine learning models that interpret numerical relationships.\n",
        "\n",
        "        * **Reduces Dimensionality:** Unlike one-hot encoding, ordinal mapping creates only one numerical column, avoiding the creation of many new columns and thus reducing dimensionality. This is efficient for features with many ordered categories.\n",
        "\n",
        "        * **Direct Interpretability:** The numerical values directly reflect the \"level\" or \"rank\" of the category, making the encoded feature more interpretable.\n",
        "\n",
        "2.  **Multi-Label One-Hot Encoding (Custom Logic):**\n",
        "    * For the `Type_of_Loan` column, which contained multiple, comma-separated loan types, a custom approach was used. It first identified all unique loan types across the dataset. Then, for each unique loan type, a new binary column (`Loan_LoanTypeName`) was created. A value of `1` in this new column indicates that the customer has that specific loan type, and `0` indicates absence.\n",
        "        * **Handles Multi-Valued Categories:** This technique is specifically designed for variables where a single observation can belong to multiple categories simultaneously (e.g., a customer can have both a \"Personal Loan\" and a \"Student Loan\"). Standard one-hot encoding assumes mutually exclusive categories, which wouldn't work here.\n",
        "\n",
        "        * **No Implicit Order:** Loan types are nominal (there's no inherent order). One-hot encoding creates independent binary features for each type, ensuring no false sense of order or magnitude is introduced.\n",
        "\n",
        "        * **Machine Learning Compatibility:** Most machine learning algorithms require numerical input. This method transforms the complex multi-label text into a compatible numerical format.\n",
        "\n",
        "3.  **One-Hot Encoding (`pd.get_dummies`):**\n",
        "    * **Technique Used:** For `Occupation` and `Payment_Behaviour`, `pd.get_dummies` was used to convert these nominal (unordered) categorical columns into numerical features. For each unique category within these columns, a new binary column is created (e.g., `Occupation_Engineer`). A value of `1` indicates the presence of that category, and `0` indicates its absence. `drop_first=True` was applied to avoid multicollinearity.\n",
        "\n",
        "        * **Handles Nominal Data:** These variables have no inherent order, and one-hot encoding treats each category as independent, preventing the model from assuming any artificial ordinal relationship between them (which would happen with label encoding).\n",
        "\n",
        "        * **Machine Learning Compatibility:** It converts categorical data into a format that machine learning algorithms can understand and process.\n",
        "        \n",
        "        * **Avoids Multicollinearity (`drop_first=True`):** By dropping the first category, it reduces perfect multicollinearity, which can be problematic for some linear models. While not strictly necessary for all models, it's good practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Make sure nltk data is downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    print(\"NLTK resources already downloaded.\")\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK resources...\")\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    print(\"NLTK resources downloaded.\")\n",
        "\n",
        "# Sample DataFrame (replace this with your actual data)\n",
        "# df = pd.read_csv(\"your_file.csv\")\n",
        "\n",
        "# Textual Data Preprocessing\n",
        "print(\"\\nTextual Data Preprocessing\")\n",
        "\n",
        "# Check if there are any object (textual) columns remaining in the DataFrame\n",
        "text_columns = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "if text_columns:\n",
        "    print(f\"Textual columns identified: {text_columns}. Proceeding with text preprocessing steps.\")\n",
        "\n",
        "    # Initialize stemmer and lemmatizer\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for col in text_columns:\n",
        "        print(f\"\\nProcessing textual column: '{col}'\")\n",
        "\n",
        "        # Expand Contractions\n",
        "        df[col] = df[col].astype(str).apply(lambda x: contractions.fix(x) if pd.notna(x) else x)\n",
        "        print(f\"  - Expanded contractions in '{col}'.\")\n",
        "\n",
        "        # Lower Casing\n",
        "        df[col] = df[col].astype(str).str.lower()\n",
        "        print(f\"  - Lowercased text in '{col}'.\")\n",
        "\n",
        "        # Remove Punctuations\n",
        "        df[col] = df[col].apply(lambda x: re.sub(r'[^\\w\\s]', '', x) if pd.notna(x) else x)\n",
        "        print(f\"  - Removed punctuations from '{col}'.\")\n",
        "\n",
        "        # Remove URLs & words/digits containing digits\n",
        "        df[col] = df[col].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+|[a-zA-Z]*\\d[a-zA-Z0-9]*', '', x) if pd.notna(x) else x)\n",
        "        print(f\"  - Removed URLs and words/digits containing digits from '{col}'.\")\n",
        "\n",
        "        # Remove Stopwords\n",
        "        df[col] = df[col].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]) if pd.notna(x) else x)\n",
        "        print(f\"  - Removed stopwords from '{col}'.\")\n",
        "\n",
        "        # Remove Extra Whitespaces\n",
        "        df[col] = df[col].apply(lambda x: re.sub(r'\\s+', ' ', x).strip() if pd.notna(x) else x)\n",
        "        print(f\"  - Removed extra whitespaces from '{col}'.\")\n",
        "\n",
        "        # Rephrase Text (skipped)\n",
        "        print(f\"  - 'Rephrase Text' is skipped due to lack of domain-specific rules or models.\")\n",
        "\n",
        "        # Tokenization\n",
        "        df[col + '_tokens'] = df[col].apply(lambda x: word_tokenize(x) if pd.notna(x) else [])\n",
        "        print(f\"  - Tokenized text in '{col}'. New column created: '{col}_tokens'.\")\n",
        "\n",
        "        # Lemmatization\n",
        "        df[col + '_normalized'] = df[col + '_tokens'].apply(\n",
        "            lambda tokens: [lemmatizer.lemmatize(word) for word in tokens] if isinstance(tokens, list) else []\n",
        "        )\n",
        "\n",
        "        # Stemming\n",
        "        df[col + '_normalized'] = df[col + '_normalized'].apply(\n",
        "            lambda tokens: [stemmer.stem(word) for word in tokens] if isinstance(tokens, list) else []\n",
        "        )\n",
        "        print(f\"  - Normalized text (lemmatization + stemming) in '{col}'. New column: '{col}_normalized'.\")\n",
        "\n",
        "        # POS Tagging\n",
        "        df[col + '_pos_tags'] = df[col + '_normalized'].apply(\n",
        "            lambda tokens: nltk.pos_tag(tokens) if isinstance(tokens, list) else []\n",
        "        )\n",
        "        print(f\"  - POS tagging complete. New column: '{col}_pos_tags'.\")\n",
        "\n",
        "        # Vectorizing (TF-IDF) - optional\n",
        "        print(f\"  - 'Vectorizing Text' is a next step (e.g., TF-IDF, Word2Vec, BERT). Applying basic TF-IDF here.\")\n",
        "        if not df[col].empty:\n",
        "            tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
        "            text_for_vectorization = df[col + '_normalized'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '').tolist()\n",
        "            if len(text_for_vectorization) > 0:\n",
        "                tfidf_matrix = tfidf_vectorizer.fit_transform(text_for_vectorization)\n",
        "                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "                df = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
        "                print(f\"  - Applied TF-IDF Vectorization. Added {tfidf_df.shape[1]} new features.\")\n",
        "            else:\n",
        "                print(f\"  - No text found in '{col}' for TF-IDF vectorization.\")\n",
        "\n",
        "    print(\"\\nTextual data preprocessing complete.\")\n",
        "else:\n",
        "    print(\"No 'object' (text) columns found in the DataFrame. Preprocessing not required.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jKVxE06BC1"
      },
      "source": [
        "Lemmatization: The code uses nltk.stem.WordNetLemmatizer to perform lemmatization on the tokenized text. This process converts words to their base or dictionary form (lemma), for example, \"running\" becomes \"run\", \"better\" becomes \"good\".\n",
        "\n",
        "* **Reduces Inflectional Forms:** Lemmatization is preferred over stemming in many cases because it reduces words to their meaningful base form, considering the word's context and part of speech. This results in actual dictionary words, which is better for interpretability and can retain more meaning.\n",
        "\n",
        "* **Improved Accuracy:** By mapping different inflections of a word to a single base form, it helps in standardizing the text without losing semantic meaning, which can improve the accuracy of subsequent analyses (like text vectorization and modeling)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text\n",
        "#mentioned in above cell \"Expand Contraction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su2EnbCh6UKQ"
      },
      "source": [
        "No text vectorization technique has been actively applied in the current code.\n",
        "\n",
        "Why Not Applied (and Why Not Chosen Yet):\n",
        "* The \"Vectorizing Text\" section in the code explicitly states that it is a conceptual step and requires a choice of technique (like TF-IDF, Count Vectorization, Word2Vec, or BERT embeddings).\n",
        "* An example of TfidfVectorizer is provided but it is commented out. This means it's there as a demonstration of how one might proceed, but it's not being executed.\n",
        "* **Reason:** The choice of text vectorization technique depends heavily on the specific machine learning model you plan to use, the nature of the text data, and the overall objectives. Without further context or explicit instruction, automatically applying one might not be optimal. It's typically the final step before feeding text features into a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "print(\"\\nFeature Manipulation (Minimizing Correlation & Creating New Features)\")\n",
        "\n",
        "# Identify highly correlated features based on previous heatmap insight\n",
        "# We know Annual_Income and Monthly_Inhand_Salary are highly correlated.\n",
        "# Choose one to keep and drop the other to minimize multicollinearity.\n",
        "# Let's keep 'Annual_Income' as it's the primary income source.\n",
        "if 'Monthly_Inhand_Salary' in df.columns and 'Annual_Income' in df.columns:\n",
        "    corr_income_salary = df['Annual_Income'].corr(df['Monthly_Inhand_Salary'])\n",
        "    if corr_income_salary > 0.9: # Threshold for high correlation\n",
        "        df = df.drop('Monthly_Inhand_Salary', axis=1)\n",
        "        print(f\"Dropped 'Monthly_Inhand_Salary' due to high correlation with 'Annual_Income' (correlation: {corr_income_salary:.2f}).\")\n",
        "else:\n",
        "    print(\"Skipped dropping 'Monthly_Inhand_Salary' as one or both columns are not present.\")\n",
        "\n",
        "\n",
        "# Also, Outstanding_Debt and Credit_Utilization_Ratio are often correlated.\n",
        "# Let's examine their correlation and decide. Keeping Credit_Utilization_Ratio as it's a normalized metric.\n",
        "if 'Outstanding_Debt' in df.columns and 'Credit_Utilization_Ratio' in df.columns:\n",
        "    corr_debt_util = df['Outstanding_Debt'].corr(df['Credit_Utilization_Ratio'])\n",
        "    if corr_debt_util > 0.7: # A common threshold for strong correlation\n",
        "        # Deciding which to drop: Credit_Utilization_Ratio is often more directly indicative of active credit usage behavior.\n",
        "        df = df.drop('Outstanding_Debt', axis=1)\n",
        "        print(f\"Dropped 'Outstanding_Debt' due to high correlation with 'Credit_Utilization_Ratio' (correlation: {corr_debt_util:.2f}).\")\n",
        "else:\n",
        "    print(\"Skipped dropping 'Outstanding_Debt' as one or both columns are not present.\")\n",
        "\n",
        "# Create new features (Feature Engineering)\n",
        "print(\"\\nCreating new features:\")\n",
        "\n",
        "# Example 1: Debt-to-Income Ratio (if not dropped)\n",
        "if 'Annual_Income' in df.columns and 'Outstanding_Debt' not in df.columns and 'Credit_Utilization_Ratio' in df.columns: # If Outstanding_Debt was dropped\n",
        "     # Can use Credit_Utilization_Ratio * Credit_Limit (if Credit_Limit was available) to approximate Debt,\n",
        "     # but for simplicity, let's assume we use what's left.\n",
        "     # If Outstanding_Debt was NOT dropped, we'd use it.\n",
        "     if 'Outstanding_Debt' in df.columns and 'Annual_Income' in df.columns:\n",
        "         df['Debt_to_Income_Ratio'] = df['Outstanding_Debt'] / (df['Annual_Income'] + 1e-6) # Add small epsilon to avoid division by zero\n",
        "         print(\"  - Created 'Debt_to_Income_Ratio'.\")\n",
        "     elif 'Credit_Utilization_Ratio' in df.columns and 'Annual_Income' in df.columns:\n",
        "         # This is a proxy, not true DTI, but captures similar concept if actual debt is gone\n",
        "         df['Utilization_Income_Ratio'] = df['Credit_Utilization_Ratio'] / (df['Annual_Income'] + 1e-6)\n",
        "         print(\"  - Created 'Utilization_Income_Ratio' (proxy for Debt-to-Income).\")\n",
        "     else:\n",
        "         print(\"  - Could not create Debt-to-Income related ratio due to missing base columns.\")\n",
        "elif 'Annual_Income' in df.columns and 'Outstanding_Debt' in df.columns: # If Outstanding_Debt was not dropped\n",
        "    df['Debt_to_Income_Ratio'] = df['Outstanding_Debt'] / (df['Annual_Income'] + 1e-6) # Add small epsilon to avoid division by zero\n",
        "    print(\"  - Created 'Debt_to_Income_Ratio'.\")\n",
        "else:\n",
        "    print(\"  - Could not create Debt-to-Income related ratio due to missing base columns.\")\n",
        "\n",
        "\n",
        "# Example 2: Monthly Debt Burden (EMI relative to Monthly Inhand Salary)\n",
        "if 'Total_EMI_per_month' in df.columns and 'Monthly_Inhand_Salary' in df.columns:\n",
        "    df['EMI_to_Salary_Ratio'] = df['Total_EMI_per_month'] / (df['Monthly_Inhand_Salary'] + 1e-6)\n",
        "    print(\"  - Created 'EMI_to_Salary_Ratio'.\")\n",
        "else:\n",
        "    print(\"  - Could not create 'EMI_to_Salary_Ratio' due to missing base columns.\")\n",
        "\n",
        "# Example 3: Payment Consistency Score (combining delay frequency and amount)\n",
        "if 'Num_of_Delayed_Payment' in df.columns and 'Delay_from_due_date' in df.columns:\n",
        "    df['Payment_Consistency'] = 1 / ((df['Num_of_Delayed_Payment'] * df['Delay_from_due_date']) + 1e-6) # Inverse of product, higher is better\n",
        "    print(\"  - Created 'Payment_Consistency'.\")\n",
        "else:\n",
        "    print(\"  - Could not create 'Payment_Consistency' due to missing base columns.\")\n",
        "\n",
        "print(\"Feature manipulation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Select your features wisely to avoid overfitting\n",
        "print(\"\\nFeature Selection\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('Credit_Score', axis=1)\n",
        "y = df['Credit_Score']\n",
        "\n",
        "# Drop any non-numeric columns (like text tokens, POS tags etc.)\n",
        "X = X.select_dtypes(include=np.number)\n",
        "\n",
        "# Drop the 'Credit_History_Age' column if it's all NaNs\n",
        "if 'Credit_History_Age' in X.columns:\n",
        "    X = X.drop('Credit_History_Age', axis=1)\n",
        "    print(\"Dropped 'Credit_History_Age' due to it containing only missing values.\")\n",
        "\n",
        "# SelectKBest feature selection\n",
        "num_features_to_select = X.shape[1] // 2  # Select half of the features\n",
        "if num_features_to_select == 0 and X.shape[1] > 0:\n",
        "    num_features_to_select = 1\n",
        "elif num_features_to_select == 0:\n",
        "    print(\"No features available for selection.\")\n",
        "else:\n",
        "    selector = SelectKBest(score_func=f_classif, k=num_features_to_select)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "    # Get selected feature names\n",
        "    selected_feature_indices = selector.get_support(indices=True)\n",
        "    selected_features = X.columns[selected_feature_indices].tolist()\n",
        "\n",
        "    X = X[selected_features]\n",
        "    print(f\"Selected {len(selected_features)} features using SelectKBest (f_classif):\")\n",
        "    print(selected_features)\n",
        "    print(f\"New X shape after feature selection: {X.shape}\")\n",
        "\n",
        "print(\"Feature selection complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "**Univariate Feature Selection method called SelectKBest with the f_classif scoring function is used:**\n",
        "\n",
        "* **SelectKBest:** This method selects the top 'k' features (in our code, k is set to half the number of current features) based on the highest scores from a statistical test. It's a simple yet effective way to remove features that have a weak relationship with the target variable.\n",
        "\n",
        "* **f_classif (ANOVA F-value):** This is the scoring function used with SelectKBest. It calculates the ANOVA F-value between each feature and the target variable.\n",
        "\n",
        "  * **Suitability for Classification:** f_classif is specifically designed for classification tasks, where the target variable is categorical (like your Credit_Score with categories 0, 1, 2). It assesses the dependency between the feature (numerical) and the categorical target variable.\n",
        "  * **Measures Group Separation:** The F-value measures the variance between the means of different target classes relative to the variance within the classes. A higher F-value indicates a stronger separation between the classes based on that feature, implying greater predictive power.\n",
        "  * **Computational Efficiency:** It's computationally inexpensive and works well as a first-pass filter to identify globally relevant features before more complex models are built."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "Based on the `f_classif` scoring (and corroborating with the insights from your correlation heatmap and pair plots), the features typically found most important for predicting `Credit_Score` are (and would likely be selected by `SelectKBest`):\n",
        "\n",
        "* **`Annual_Income`:**\n",
        "    * **Why Important:** Higher income generally indicates a greater capacity to repay debts, which is a fundamental aspect of creditworthiness. Our pair plots clearly showed a positive correlation between income and `Credit_Score`.\n",
        "* **`Outstanding_Debt` or `Credit_Utilization_Ratio`:** (One of these would likely be kept due to high correlation, e.g., `Credit_Utilization_Ratio`)\n",
        "    * **Why Important:** These metrics directly reflect a person's current debt burden and how much of their available credit they are using. High debt or utilization indicates higher risk. The correlation heatmap showed a strong negative correlation with `Credit_Score`.\n",
        "* **`Interest_Rate`:**\n",
        "    * **Why Important:** The interest rate a person is offered is often a direct reflection of their perceived credit risk. Lower interest rates are given to more creditworthy individuals. The pair plot revealed a very strong inverse relationship with `Credit_Score`.\n",
        "* **`Num_of_Delayed_Payment` and `Delay_from_due_date`:**\n",
        "    * **Why Important:** These features directly quantify past negative payment behavior. Frequent or longer delays are strong indicators of higher credit risk. Our specific charts highlighted how these relate to overall credit score categories.\n",
        "* **`Total_EMI_per_month` and `Monthly_Balance`:**\n",
        "    * **Why Important:** These reflect a customer's ongoing financial commitments and liquidity. A high EMI relative to income, or a low monthly balance, can indicate financial strain and higher risk.\n",
        "* **`Credit_Mix`:**\n",
        "    * **Why Important:** A diverse and healthy credit mix (e.g., a good blend of installment loans and revolving credit) is often seen favorably by creditors as it demonstrates responsible management of different credit types.\n",
        "\n",
        "These features are crucial because they directly impact a lender's assessment of a borrower's ability and willingness to repay, which are the core components of a credit score. `SelectKBest` with `f_classif` would identify these features as having the most statistically significant differences in their means across the 'Poor', 'Standard', and 'Good' credit score groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRCgOLR6WohS"
      },
      "source": [
        "Yes, based on the nature of typical financial datasets and the common requirements of machine learning models, **I do think the data needs to be transformed.**\n",
        "\n",
        "In the provided code, **logarithmic transformation** specifically, `np.log1p()` is being used.\n",
        "\n",
        "**Reason:**\n",
        "\n",
        "* **Handling Skewed Distributions:** Many continuous numerical features in real-world datasets, especially financial ones like `Annual_Income`, `Outstanding_Debt`, `Total_EMI_per_month`, and `Monthly_Balance`, often exhibit **skewed distributions** (e.g., right-skewed, where a large number of values are concentrated on the lower end, and a long tail extends towards higher values).\n",
        "\n",
        "* **Meeting Model Assumptions:** Many machine learning algorithms (particularly linear models, but also some others) perform optimally when input features are **normally or symmetrically distributed**. Skewed data can violate these assumptions, leading to:\n",
        "    * Suboptimal model performance.\n",
        "    * Difficulty for the model to capture patterns effectively from the feature.\n",
        "    * Features with large ranges dominating the learning process if not scaled correctly.\n",
        "\n",
        "* **Stabilizing Variance:** Log transformations can help to stabilize the variance of features, making patterns more consistent across the range of the data.\n",
        "\n",
        "* **Graceful Handling of Zeroes:** I specifically used `np.log1p(x)` (which computes $\\log(1+x)$) instead of `np.log(x)`. This is crucial because `np.log(0)` is undefined. Many financial features can naturally have zero values (e.g., zero `Amount_invested_monthly`, zero `Outstanding_Debt`). `log1p` handles these zero values gracefully by mapping them to `0`, preventing errors and preserving data integrity.\n",
        "\n",
        "By applying `log1p` to these potentially skewed numerical columns, we aim to make their distributions more symmetrical, which generally helps machine learning models learn more robust and accurate relationships from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data\n",
        "print(\"\\nData Transformation\")\n",
        "\n",
        "# Identify numerical columns for transformation (excluding binary/encoded and target)\n",
        "# We select columns that are continuous and likely to be skewed\n",
        "\n",
        "transform_cols = [\n",
        "    'Annual_Income', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
        "    'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
        "    'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries',\n",
        "    'Outstanding_Debt' if 'Outstanding_Debt' in X.columns else None, # Include if not dropped\n",
        "    'Credit_Utilization_Ratio', 'Credit_History_Age', 'Total_EMI_per_month',\n",
        "    'Amount_invested_monthly', 'Monthly_Balance', 'Age',\n",
        "    'Debt_to_Income_Ratio' if 'Debt_to_Income_Ratio' in X.columns else None,\n",
        "    'Utilization_Income_Ratio' if 'Utilization_Income_Ratio' in X.columns else None,\n",
        "    'EMI_to_Salary_Ratio' if 'EMI_to_Salary_Ratio' in X.columns else None,\n",
        "    'Payment_Consistency' if 'Payment_Consistency' in X.columns else None\n",
        "]\n",
        "transform_cols = [col for col in transform_cols if col is not None and col in X.columns] # Filter for existing\n",
        "\n",
        "if not transform_cols:\n",
        "    print(\"No numerical columns found for transformation.\")\n",
        "else:\n",
        "    print(\"Applying log transformation (log1p) to skewed numerical columns:\")\n",
        "    for col in transform_cols:\n",
        "        # Check for skewness before transforming (optional, but good practice)\n",
        "        # Here we'll apply log1p as a general transformation for potentially skewed data\n",
        "        if (X[col] >= 0).all(): # log1p only works for non-negative values\n",
        "            X[col] = np.log1p(X[col]) # log1p handles 0 values gracefully (log(1+x))\n",
        "            print(f\"  - Applied log1p transformation to '{col}'.\")\n",
        "        else:\n",
        "            print(f\"  - Skipping log1p for '{col}' due to negative values.\")\n",
        "\n",
        "print(\"Data transformation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "print(\"\\nData Scaling (StandardScaler)\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply scaling to the selected features (X)\n",
        "# Ensure X contains only numerical features before scaling\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert scaled data back to DataFrame with original column names\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "print(\"Data scaling complete. Features are now standardized.\")\n",
        "print(f\"Scaled X head:\\n{X.head()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPRSAUtWXVgw"
      },
      "source": [
        "**Standard Scaling** as the method to scale data is being used.\n",
        "\n",
        "* **Method Used:** `sklearn.preprocessing.StandardScaler`\n",
        "\n",
        "* **Reason:**\n",
        "    * **Standardization:** StandardScaler transforms the data such that it has a **mean of 0** and a **standard deviation of 1**. This process is also known as standardization.\n",
        "\n",
        "    * **Equal Contribution:** Many machine learning algorithms (especially those based on distance calculations, like K-Nearest Neighbors, Support Vector Machines, or those using gradient descent, like neural networks and logistic regression) are sensitive to the scale and magnitude of input features. If features have vastly different ranges, features with larger values might disproportionately influence the model's objective function or distance calculations.\n",
        "\n",
        "    * **Prevents Dominance:** Standard scaling ensures that all features contribute approximately equally to the model's performance, preventing features with larger numerical ranges from dominating those with smaller ranges.\n",
        "\n",
        "    * **Improved Convergence:** For iterative optimization algorithms (like gradient descent used in many models), scaling can lead to faster and more stable convergence, as it puts features on a similar scale.\n",
        "    \n",
        "    * **Common and Effective:** Standard scaling is a widely used and highly effective preprocessing step for a broad range of machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "Based on the steps performed so far, **dimensionality reduction (specifically PCA, as implemented in the code) is conditionally included, meaning it *might* be needed, but not necessarily always.**\n",
        "\n",
        "**Why Dimensionality Reduction Might Be Needed:**\n",
        "\n",
        "1.  **Curse of Dimensionality:** After one-hot encoding and multi-label encoding of categorical variables like `Occupation`, `Type_of_Loan`, and `Payment_Behaviour`, your dataset can end up with a **very large number of features**. A high number of features (dimensions) can lead to:\n",
        "    * **Increased Computational Cost:** Models take longer to train and require more memory.\n",
        "    * **Overfitting:** With too many features, models might learn noise in the training data rather than true underlying patterns, performing poorly on unseen data.\n",
        "    * **Reduced Interpretability:** It becomes harder to understand which specific features are driving model predictions.\n",
        "    * **Sparse Data:** In high-dimensional spaces, data points become very sparse, making it harder for algorithms to find meaningful relationships.\n",
        "\n",
        "2.  **Multicollinearity:** While we addressed high correlation by dropping features like `Monthly_Inhand_Salary` and `Outstanding_Debt`, complex interdependencies can still exist among the many encoded binary features (e.g., different loan types or occupations). PCA can help to combine these correlated features into a smaller set of uncorrelated components.\n",
        "\n",
        "**Why Dimensionality Reduction Might Not Be Strictly Necessary:**\n",
        "\n",
        "1.  **Feature Selection Already Performed:** We've already used `SelectKBest` to reduce the number of features by selecting only the most statistically relevant ones. If this step sufficiently reduces the feature count (e.g., below the 30-feature threshold set in the code), then further dimensionality reduction might not be critical.\n",
        "2.  **Loss of Interpretability:** PCA transforms the original features into new, uncorrelated \"principal components.\" These components are linear combinations of the original features and are often less interpretable than the original features themselves. If understanding the direct impact of each original feature is paramount for business insights, then avoiding PCA (or using it cautiously) is preferable.\n",
        "3.  **Model Robustness:** Some machine learning models (e.g., tree-based models like Random Forests or Gradient Boosting Machines) are naturally more robust to high dimensionality and multicollinearity, making PCA less critical for them compared to linear models or SVMs.\n",
        "\n",
        "**How the Code Addresses It:**\n",
        "\n",
        "The code wisely incorporates PCA as a **conditional step**. It checks if the number of features `X.shape[1]` is greater than 30 after the feature selection process. This threshold (`> 30`) is a pragmatic heuristic. If the number of features is still high, it applies PCA to retain 95% of the explained variance, effectively reducing the dimensionality while preserving most of the original information. If the feature count is already manageable, it skips PCA to preserve interpretability.\n",
        "\n",
        "In summary, dimensionality reduction is a valuable tool when facing a high number of features, but its necessity depends on the specific dataset, the chosen machine learning model, and the trade-off between computational efficiency/overfitting and interpretability. The current code provides a flexible and reasonable approach to this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "outputs": [],
      "source": [
        "# Dimensionality Reduction (If needed)\n",
        "print(\"\\nDimensionality Reduction (PCA)\")\n",
        "\n",
        "# Determine if PCA is 'needed'. This is subjective and depends on feature count and performance.\n",
        "# As a heuristic, if we still have more than, say, 20 features, or if performance is a concern,\n",
        "# PCA can be considered. For now, we'll keep it optional or apply it if many features remain.\n",
        "\n",
        "# Let's set a threshold, e.g., apply PCA if more than 30 features\n",
        "if X.shape[1] > 30:\n",
        "    print(f\"Current number of features ({X.shape[1]}) is high. Considering PCA.\")\n",
        "    # Choose number of components. A common approach is to explain a certain variance (e.g., 95%)\n",
        "    # or select a fixed number. Let's aim for 95% variance explained for demonstration.\n",
        "    pca = PCA(n_components=0.95) # Retain 95% of variance\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Create new DataFrame for PCA components\n",
        "    pca_cols = [f'PC_{i+1}' for i in range(X_pca.shape[1])]\n",
        "    X = pd.DataFrame(X_pca, columns=pca_cols, index=X.index)\n",
        "\n",
        "    print(f\"PCA applied. Reduced dimensions from {pca.n_features_} to {pca.n_components_}.\")\n",
        "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.2f}\")\n",
        "    print(f\"New X shape after PCA: {X.shape}\")\n",
        "else:\n",
        "    print(f\"Dimensionality reduction (PCA) not applied as current number of features ({X.shape[1]}) is manageable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr75IDuEM7t"
      },
      "source": [
        "In the provided code, the dimensionality reduction technique used is **Principal Component Analysis (PCA)**.\n",
        "\n",
        "* **Technique Used:** `sklearn.decomposition.PCA`\n",
        "\n",
        "* **Reason (If Applied to the Dataset):**\n",
        "\n",
        "    1.  **Handles Multicollinearity:** After extensive encoding (especially multi-label and one-hot encoding), datasets often end up with many features that are correlated with each other. PCA effectively transforms these correlated features into a smaller set of **uncorrelated (orthogonal) principal components**. This helps in reducing redundancy in the data.\n",
        "\n",
        "    2.  **Variance Preservation:** PCA aims to retain as much of the original data's variance as possible in the reduced set of dimensions. By setting `n_components=0.95` in the code, we instruct PCA to find the minimum number of components that collectively explain at least 95% of the total variance in the original features. This means we compress the data while losing minimal information.\n",
        "\n",
        "    3.  **Combats the Curse of Dimensionality:** When you have a very high number of features (as can happen after one-hot encoding), machine learning models can struggle due to the \"curse of dimensionality,\" leading to longer training times, increased risk of overfitting, and sparser data. PCA helps mitigate these issues by projecting the data into a lower-dimensional space.\n",
        "\n",
        "    4.  **Applicability to Numerical Data:** PCA works with numerical data. Since all our features have been transformed into numerical types (through original numerical columns, ordinal mapping, multi-label encoding, and one-hot encoding), PCA is directly applicable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "print(\"\\nData Splitting (Train/Test Split)\")\n",
        "\n",
        "# Define features (X) and target (y) again, ensuring they reflect the latest transformations\n",
        "# X and y are already defined and updated in previous steps\n",
        "\n",
        "# Splitting ratio: 80% training, 20% testing is a common and wise choice.\n",
        "# stratify=y is crucial for imbalanced datasets to maintain target distribution in both sets.\n",
        "# random_state for reproducibility.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Data split into training and testing sets:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nCredit Score distribution in original data:\")\n",
        "print(y.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nCredit Score distribution in training set:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nCredit Score distribution in test set:\")\n",
        "print(y_test.value_counts(normalize=True))\n",
        "\n",
        "print(\"Data splitting complete (stratified).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "Used an **80/20 data splitting ratio**.\n",
        "\n",
        "* **Ratio Used:**\n",
        "\n",
        "    * **80% for the training set (`train_size=0.8` or implied by `test_size=0.2`)**\n",
        "\n",
        "    * **20% for the testing set (`test_size=0.2`)**\n",
        "\n",
        "* **Why:**\n",
        "   \n",
        "    1.  **Common Practice:** An 80/20 split (or similar variations like 70/30, 75/25) is a widely accepted and common practice in machine learning. It strikes a good balance between:\n",
        "        \n",
        "        * **Sufficient Training Data:** Providing enough data for the model to learn complex patterns and relationships effectively. With 80% of 100,000 records, the model gets 80,000 samples for training.\n",
        "        \n",
        "        * **Representative Test Data:** Reserving a reasonably sized, unseen portion (20%, or 20,000 records) to evaluate the model's generalization performance on new data. This helps in assessing how well the model will perform in the real world.\n",
        "\n",
        "    2.  **Dataset Size:** For a dataset of 100,000 records, an 80/20 split provides ample data for both training and testing, making it statistically robust. Smaller datasets might warrant different strategies (e.g., k-fold cross-validation).\n",
        "\n",
        "    3.  **Stratified Sampling (`stratify=y`):** This is a crucial addition to the splitting.\n",
        "        \n",
        "        * **Why used with `stratify`:** Your `Credit_Score` target variable is likely imbalanced (as confirmed by earlier analysis). `stratify=y` ensures that the proportion of each `Credit_Score` class (Poor, Standard, Good) is approximately the same in both the training and testing sets as it is in the original dataset. This prevents a situation where, by random chance, one set might have significantly more or fewer samples of a particular class, leading to biased training or an unreliable evaluation of the model's performance on minority classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "Yes, based on the nature of the `Credit_Score` distribution, **the dataset is indeed considered imbalanced.**\n",
        "\n",
        "1.  **Uneven Class Distribution:** In your previous `df.info()` output and particularly from the pair plot visualization (where you could see the density of 'Standard' scores was visually higher than 'Poor' or 'Good' scores), it's evident that the classes within your target variable, `Credit_Score` (0: Poor, 1: Standard, 2: Good), are **not equally represented**. Typically, you would see this imbalance clearly by looking at the `.value_counts()` or `.value_counts(normalize=True)` of the `Credit_Score` column. While I don't have the exact numbers from a `.value_counts()` output right now, it's a common characteristic of credit scoring datasets for the 'Standard' class to be the majority, and 'Poor' or 'Good' classes to be minorities.\n",
        "\n",
        "2.  **Impact on Model Training:** When a dataset is imbalanced, machine learning models tend to be **biased towards the majority class**.\n",
        "\n",
        "    * They might achieve high overall accuracy by simply predicting the majority class for most samples.\n",
        "\n",
        "    * However, their performance on the minority classes (which are often the most critical ones, e.g., correctly identifying 'Poor' credit scores to prevent defaults, or 'Good' scores for targeted offers) will be poor.\n",
        "    * This leads to models that don't generalize well to the real world where minority class instances do occur and need to be correctly identified.\n",
        "\n",
        "Therefore, addressing this imbalance is a crucial step in preparing the data for effective model training, which is why the code includes the SMOTE (Synthetic Minority Over-sampling Technique) step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Handling Imbalanced Dataset (If needed)\n",
        "print(\"\\nHandling Imbalanced Dataset\")\n",
        "\n",
        "# Check the current distribution of the target variable in the training set\n",
        "print(\"Credit Score distribution in training set BEFORE imbalance handling:\")\n",
        "print(y_train.value_counts())\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "# Apply SMOTE to balance the training set\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nImbalance handling complete (SMOTE applied to training data).\")\n",
        "print(\"Credit Score distribution in training set AFTER imbalance handling:\")\n",
        "print(y_train_resampled.value_counts())\n",
        "print(y_train_resampled.value_counts(normalize=True))\n",
        "\n",
        "print(\"Imbalanced dataset handling complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "In the provided code, the technique used to handle the imbalanced dataset is **SMOTE (Synthetic Minority Over-sampling Technique)**.\n",
        "\n",
        "* **Technique Used:** `imblearn.over_sampling.SMOTE`\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "    1.  **Addresses Imbalance by Oversampling:** SMOTE works by creating synthetic (artificial) samples for the minority class(es). Instead of simply duplicating existing minority samples (which can lead to overfitting), SMOTE generates new, synthetic samples that are \"similar\" to the existing minority samples but not identical. It does this by picking a sample from the minority class and then considering its k-nearest neighbors. It then creates new synthetic samples at random points between the chosen sample and its neighbors.\n",
        "\n",
        "    2.  **Prevents Overfitting from Simple Duplication:** If we were to just duplicate the minority class samples, the model might overfit to those exact duplicated instances. SMOTE's synthetic generation helps to create a more generalized representation of the minority class, reducing this risk.\n",
        "\n",
        "    3.  **Does Not Discard Data:** Unlike undersampling techniques (which reduce the majority class), SMOTE retains all information from the majority class, which can be important if the majority class holds valuable patterns.\n",
        "\n",
        "    4.  **Applied Only to Training Data:** Critically, SMOTE is applied *only* to the `X_train` and `y_train` sets. This is a standard and essential practice to prevent **data leakage**. If SMOTE were applied before the train-test split, the synthetic samples generated in the training set might be based on information from the test set, leading to an artificially inflated (and misleading) model performance during evaluation.\n",
        "\n",
        "    5.  **Balances Class Distribution:** The primary goal is to balance the class distribution in the training set, allowing the machine learning model to learn from a more representative proportion of all classes. This helps the model become less biased towards the majority class and improves its ability to correctly classify minority classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# @title Logistic Regression\n",
        "\n",
        "# ML Model - 1 Implementation\n",
        "\n",
        "print(\"--- Implementing Logistic Regression ---\")\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "# max_iter is increased for convergence, especially with scaled data and potentially more features.\n",
        "# solver='liblinear' is good for small datasets and L1/L2 regularization.\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
        "\n",
        "print(\"Logistic Regression model initialized.\")\n",
        "\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "print(\"\\n--- Fitting Logistic Regression Model ---\")\n",
        "\n",
        "# Fit the model on the resampled training data\n",
        "logistic_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Logistic Regression model fitted successfully to the training data.\")\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the Model\n",
        "print(\"\\n--- Making Predictions with Logistic Regression ---\")\n",
        "\n",
        "# Predict classes\n",
        "y_pred_logistic = logistic_model.predict(X_test)\n",
        "\n",
        "# Predict probabilities (needed for ROC AUC)\n",
        "y_pred_proba_logistic = logistic_model.predict_proba(X_test)\n",
        "\n",
        "print(\"Predictions made on the test set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    roc_curve,\n",
        "    auc\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# --- Evaluating Logistic Regression Model ---\n",
        "print(\"\\n--- Evaluating Logistic Regression Model ---\")\n",
        "\n",
        "# Define class labels\n",
        "credit_score_labels = ['Poor', 'Standard', 'Good']\n",
        "n_classes = len(credit_score_labels)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_logistic, target_names=credit_score_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_logistic = confusion_matrix(y_test, y_pred_logistic)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_logistic, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=credit_score_labels,\n",
        "            yticklabels=credit_score_labels)\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.xlabel('Predicted Credit Score')\n",
        "plt.ylabel('True Credit Score')\n",
        "plt.show()\n",
        "\n",
        "# Overall Accuracy Score\n",
        "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
        "print(f\"\\nOverall Accuracy: {accuracy_logistic:.4f}\")\n",
        "\n",
        "# ROC AUC Curve (One-vs-Rest)\n",
        "print(\"\\nGenerating Multi-class ROC AUC Curve (One-vs-Rest)...\")\n",
        "\n",
        "# Binarize the true labels\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['red', 'blue', 'green']  # Colors for classes: Poor, Standard, Good\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba_logistic[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "             label=f'ROC curve of class {credit_score_labels[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Logistic Regression (One-vs-Rest)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nLogistic Regression model evaluation complete with visualizations.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGRJHhDCDVbw"
      },
      "source": [
        "**Inference for Base Logistic Regression Model**\n",
        "\n",
        "**Overall Performance:**\n",
        "The model achieved an **Overall Accuracy of 0.6356 (63.56%)**. While this might seem moderate, it's crucial to look beyond overall accuracy, especially in imbalanced datasets.\n",
        "\n",
        "**Detailed Performance by Class (from Classification Report and Confusion Matrix):**\n",
        "\n",
        "* **Class 'Standard' (Credit Score = 1):**\n",
        "    * **Best Performance:** The model performs best on the 'Standard' class.\n",
        "        * **Precision (0.70):** When the model predicts 'Standard', it's correct 70% of the time.\n",
        "        * **Recall (0.64):** It correctly identifies 64% of all actual 'Standard' cases.\n",
        "        * **F1-score (0.67):** A good balance between precision and recall for this class.\n",
        "    * **Reason:** This is expected, as 'Standard' is likely the majority class, and models tend to learn patterns of the majority class more effectively if not specifically tuned for imbalance (even with initial SMOTE, base models might still lean towards it). The confusion matrix shows 5468 'Standard' cases correctly predicted as 'Standard'.\n",
        "\n",
        "* **Class 'Poor' (Credit Score = 0):**\n",
        "    * **Moderate Performance:**\n",
        "        * **Precision (0.57):** When predicting 'Poor', it's correct 57% of the time.\n",
        "        * **Recall (0.71):** It's good at *identifying* 'Poor' cases (71% of actual 'Poor' cases are caught). This is a relatively high recall, which is important for identifying high-risk customers.\n",
        "        * **F1-score (0.63):** A decent balance.\n",
        "    * **Reason:** The high recall for 'Poor' is a positive sign, indicating the model is fairly good at capturing truly 'Poor' credit scores. However, the precision suggests that when it *says* someone is 'Poor', there's still a significant chance (43%) they are not. From the confusion matrix, out of 5755 actual 'Poor' cases, 4240 were correctly classified, but 679 were misclassified as 'Standard' and 880 as 'Good'.\n",
        "\n",
        "* **Class 'Good' (Credit Score = 2):**\n",
        "    * **Weakest Performance:**\n",
        "        * **Precision (0.50):** Only 50% of the time that the model predicts 'Good', it's actually correct.\n",
        "        * **Recall (0.84):** It's excellent at *identifying* actual 'Good' cases (84% recalled!). This is surprisingly high given the likely minority status.\n",
        "        * **F1-score (0.63):** This F1-score is driven up by high recall, but the low precision indicates many false positives for this class.\n",
        "    * **Reason:** The very high recall but low precision for 'Good' suggests that the model is being overly optimistic and classifying many 'Poor' or 'Standard' customers as 'Good' (false positives). This is visible in the confusion matrix where only 3004 out of 3566 actual 'Good' cases are correctly classified, but it has a high number of predictions into 'Good' from 'Poor' (880) and 'Standard' (2141). This \"over-prediction\" of the 'Good' class might be a side effect of the base model's default behavior or the interaction with SMOTE without further hyperparameter tuning.\n",
        "\n",
        "**ROC AUC Curves:**\n",
        "\n",
        "* **Class 'Poor' (Area = 0.80):** A good AUC score, indicating the model has a strong ability to distinguish 'Poor' cases from others.\n",
        "* **Class 'Standard' (Area = 0.74):** A decent AUC, showing moderate discriminative power for the 'Standard' class.\n",
        "* **Class 'Good' (Area = 0.67):** The lowest AUC, indicating the weakest ability to distinguish 'Good' cases from others, supporting the low precision observed for this class. The curve for 'Good' is closer to the random classifier line compared to 'Poor' and 'Standard'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from scipy.stats import uniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Implementing Logistic Regression with RandomizedSearchCV ---\n",
        "print(\"--- Implementing Logistic Regression with RandomizedSearchCV ---\")\n",
        "\n",
        "# Define the base logistic regression model\n",
        "base_logistic_model = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_distributions = {\n",
        "    'C': uniform(loc=0.01, scale=100),               # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],                         # Regularization type\n",
        "    'solver': ['liblinear', 'saga'],                 # Solvers that support both l1 and l2\n",
        "    'class_weight': [None, 'balanced']               # Handle imbalance (optional with SMOTE)\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search_logistic = RandomizedSearchCV(\n",
        "    estimator=base_logistic_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    cv=2,\n",
        "    scoring='f1_weighted',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"RandomizedSearchCV setup for Logistic Regression optimization.\")\n",
        "\n",
        "# --- Fit the model using RandomizedSearchCV ---\n",
        "print(\"\\n--- Fitting Logistic Regression Model with RandomizedSearchCV ---\")\n",
        "random_search_logistic.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Retrieve the best model from RandomizedSearchCV\n",
        "best_logistic_model = random_search_logistic.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "print(random_search_logistic.best_params_)\n",
        "print(\"\\nOptimized Logistic Regression model fitted successfully.\")\n",
        "\n",
        "# --- Make predictions using the best model ---\n",
        "print(\"\\n--- Making Predictions with Optimized Logistic Regression ---\")\n",
        "y_pred_logistic = best_logistic_model.predict(X_test)\n",
        "y_pred_proba_logistic = best_logistic_model.predict_proba(X_test)\n",
        "print(\"Predictions made on the test set using the optimized model.\")\n",
        "\n",
        "# --- Evaluate the Model ---\n",
        "print(\"\\n--- Evaluating Optimized Logistic Regression Model ---\")\n",
        "\n",
        "# Define class labels\n",
        "credit_score_labels = ['Poor', 'Standard', 'Good']\n",
        "n_classes = len(credit_score_labels)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_logistic, target_names=credit_score_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_logistic)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=credit_score_labels,\n",
        "            yticklabels=credit_score_labels)\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy Score\n",
        "accuracy = accuracy_score(y_test, y_pred_logistic)\n",
        "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# --- ROC AUC Curve (One-vs-Rest) ---\n",
        "print(\"\\nGenerating Multi-class ROC AUC Curve (One-vs-Rest)...\")\n",
        "\n",
        "# Binarize true labels\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "# ROC Curve Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba_logistic[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "             label=f'ROC curve of class {credit_score_labels[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Logistic Regression (One-vs-Rest)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nOptimized Logistic Regression evaluation complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm-51DrDFLFa"
      },
      "source": [
        "**Inference for Optimized Logistic Regression Model**\n",
        "\n",
        "**Best Hyperparameters Found:** (Assuming these were printed in your console; for this inference, we'll focus on the performance metrics improvement). The optimization aimed to find the best balance of `C`, `penalty`, `solver`, and `class_weight` (if used in the search) to maximize the `f1_weighted` score.\n",
        "\n",
        "**Overall Performance:**\n",
        "The **Overall Accuracy increased slightly to 0.6434 (64.34%)** from 0.6356. This modest increase suggests that hyperparameter tuning had some positive impact on overall correctness.\n",
        "\n",
        "**Detailed Performance by Class (from Classification Report and Confusion Matrix):**\n",
        "\n",
        "* **Class 'Standard' (Credit Score = 1):**\n",
        "    * **Precision (0.70):** Remains consistent. When the model predicts 'Standard', it's correct 70% of the time.\n",
        "    * **Recall (0.63):** Slightly decreased from 0.64. It correctly identifies 63% of all actual 'Standard' cases.\n",
        "    * **F1-score (0.66):** A minor drop from 0.67.\n",
        "    * **Confusion Matrix:** 5740 true 'Standard' predictions. Misclassifications of 'Standard' into 'Poor' (2825) and 'Good' (2070) are still significant but show some slight changes compared to the base model.\n",
        "\n",
        "* **Class 'Poor' (Credit Score = 0):**\n",
        "    * **Precision (0.58):** Improved from 0.57. When the model predicts 'Poor', it's correct 58% of the time, a small but positive gain.\n",
        "    * **Recall (0.72):** Also slightly improved from 0.71. It correctly identifies 72% of all actual 'Poor' cases.\n",
        "    * **F1-score (0.64):** Improved from 0.63.\n",
        "    * **Confusion Matrix:** 4153 'Poor' cases correctly predicted. The number of 'Poor' misclassified as 'Standard' (779) and 'Good' (867) have slightly shifted. The model has become marginally better at recalling true 'Poor' cases and slightly more precise when predicting 'Poor'.\n",
        "\n",
        "* **Class 'Good' (Credit Score = 2):**\n",
        "    * **Significant Improvement in Precision!**\n",
        "        * **Precision (0.58):** This is a notable improvement from 0.50 in the base model. When the optimized model predicts 'Good', it's now correct 58% of the time, reducing the number of false positives for the 'Good' class.\n",
        "        * **Recall (0.83):** Remains high at 83% (very slight decrease from 0.84). It's still excellent at recalling actual 'Good' cases.\n",
        "        * **F1-score (0.68):** A significant increase from 0.63, indicating a much better balance between precision and recall for this class.\n",
        "    * **Confusion Matrix:** 2975 actual 'Good' cases correctly predicted. The crucial change here is the reduction in misclassifications from 'Standard' to 'Good' (from 2141 to 2070) and from 'Poor' to 'Good' (from 880 to 867). While still present, the improvement in precision for 'Good' suggests that the model is making fewer overly optimistic predictions.\n",
        "\n",
        "**ROC AUC Curves:**\n",
        "\n",
        "* **Class 'Poor' (Area = 0.81):** Increased from 0.80, indicating slightly better discriminatory power.\n",
        "* **Class 'Standard' (Area = 0.75):** Increased from 0.74, showing a marginal improvement.\n",
        "* **Class 'Good' (Area = 0.68):** Increased from 0.67, confirming the improved discrimination for this challenging class. The curve is marginally further from the random classifier line.\n",
        "\n",
        "**Summary and Business Impact of Optimization:**\n",
        "\n",
        "The hyperparameter optimization had a positive impact on the Logistic Regression model, particularly in improving the performance on the 'Good' credit score class.\n",
        "\n",
        "* **Positive Impact:**\n",
        "    * **Reduced False Positives for 'Good':** The most significant gain is in the precision for the 'Good' class. This means Paisabazaar will be more accurate when identifying genuinely creditworthy customers. This directly translates to:\n",
        "        * **Lower Risk:** Fewer loans will be approved for individuals who were wrongly identified as 'Good'.\n",
        "        * **Better Targeting:** More precise identification of 'Good' customers allows for more effective targeting of premium products or lower interest rates, retaining valuable clients.\n",
        "    * **Slight Improvements Across the Board:** While subtle, the improvements in AUC and F1-scores for 'Poor' and 'Standard' classes indicate a generally more robust model across all credit categories.\n",
        "* **Continued Challenge:** Despite the improvements, misclassifications still occur. The model still misclassifies a notable number of 'Standard' customers as 'Poor' (2825) and 'Good' (2070), and similarly for 'Poor' (779 as Standard, 867 as Good). This suggests there's still room for improvement, possibly by exploring more complex models or additional feature engineering.\n",
        "\n",
        "In conclusion, the optimization successfully refined the Logistic Regression model, making its predictions, especially for the 'Good' credit score, more reliable. This is a step in the right direction for Paisabazaar to make more informed and less risky lending decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "**Randomized Search Cross-Validation (`RandomizedSearchCV`)** as the hyperparameter optimization technique: explores a defined range or distribution of hyperparameter values by **randomly sampling** a fixed number of combinations.\n",
        "\n",
        "* **Why:**\n",
        "    \n",
        "    1.  **Efficiency over Exhaustive Search:** While `GridSearchCV` is thorough, it can become computationally very expensive and time-consuming, especially with multiple hyperparameters and a wide range of values. `RandomizedSearchCV` allows us to control the computational budget by specifying `n_iter` (the number of random combinations to try). This means we can explore a wider search space more efficiently within a given time constraint.\n",
        "    \n",
        "    2.  **Effectiveness in Finding Good Solutions:** Research and practice have shown that for many problems, `RandomizedSearchCV` can often find a set of hyperparameters that performs just as well as (or sometimes even better than) `GridSearchCV` within the same or even less time. This is because important hyperparameters often have a much wider range of optimal values than might be intuitively defined in a fixed grid.\n",
        "    \n",
        "    3.  **Cross-Validation for Robustness:** Like `GridSearchCV`, `RandomizedSearchCV` incorporates cross-validation (`cv=5` in the code). This ensures that the model's performance for each sampled hyperparameter combination is evaluated on multiple folds of the training data, providing a more robust and reliable estimate of its generalization ability and preventing overfitting to a single validation set.\n",
        "    \n",
        "    4.  **Practicality for Initial Tuning:** It's a practical and effective method for the initial phase of hyperparameter tuning, helping to quickly narrow down promising regions of the hyperparameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "Yes, based on the \"Comparison of Base vs. Optimized Logistic Regression Models\" chart and the accompanying metrics, **there has been an improvement** after hyperparameter optimization.\n",
        "\n",
        "\n",
        "\n",
        "| Metric                | Base Logistic Regression | Optimized Logistic Regression | Change     |\n",
        "| :-------------------- | :----------------------- | :---------------------------- | :--------- |\n",
        "| **Accuracy** | 0.6356                   | **0.6434** | **+0.0078** |\n",
        "| Precision (Macro Avg) | 0.6358                   | **0.6371** | **+0.0013** |\n",
        "| Recall (Macro Avg)    | 0.6959                   | **0.6967** | **+0.0008** |\n",
        "| F1-Score (Macro Avg)  | 0.6346                   | **0.6408** | **+0.0062** |\n",
        "| F1-Score (Weighted Avg)| 0.6361                   | **0.6453** | **+0.0092** |\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "* **Overall Accuracy:** There's a small but positive increase in overall accuracy from **0.6356 to 0.6434**.\n",
        "\n",
        "* **F1-Score (Weighted Avg):** This is a very important metric, especially for imbalanced datasets, as it accounts for both precision and recall across all classes and weights them by class support. It shows the most significant improvement, increasing from **0.6361 to 0.6453**. This indicates a better balanced performance across all classes, considering their proportions.\n",
        "\n",
        "* **F1-Score (Macro Avg):** Also shows a positive increase from **0.6346 to 0.6408**, suggesting improved balance across classes when considering them equally important (unweighted average).\n",
        "\n",
        "* **Precision (Macro Avg) and Recall (Macro Avg):** Both also show slight positive shifts, contributing to the F1-score improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YhYsMCiGWpT"
      },
      "outputs": [],
      "source": [
        "# @title Compare Results of Base vs. Optimized Logistic Regression Models\n",
        "print(\"--- Comparing Base Logistic Regression vs. Optimized Logistic Regression ---\")\n",
        "\n",
        "# Define target names/labels for clear reporting\n",
        "credit_score_labels = ['Poor', 'Standard', 'Good']\n",
        "\n",
        "# --- Metrics for Base Logistic Regression Model ---\n",
        "report_base = classification_report(y_test, y_pred_logistic, output_dict=True)\n",
        "accuracy_base = accuracy_score(y_test, y_pred_logistic)\n",
        "precision_macro_base = report_base['macro avg']['precision']\n",
        "recall_macro_base = report_base['macro avg']['recall']\n",
        "f1_macro_base = report_base['macro avg']['f1-score']\n",
        "f1_weighted_base = report_base['weighted avg']['f1-score']\n",
        "\n",
        "\n",
        "# --- Metrics for Optimized Logistic Regression Model ---\n",
        "report_optimized = classification_report(y_test, y_pred_logistic_optimized, output_dict=True)\n",
        "accuracy_optimized = accuracy_score(y_test, y_pred_logistic_optimized)\n",
        "precision_macro_optimized = report_optimized['macro avg']['precision']\n",
        "recall_macro_optimized = report_optimized['macro avg']['recall']\n",
        "f1_macro_optimized = report_optimized['macro avg']['f1-score']\n",
        "f1_weighted_optimized = report_optimized['weighted avg']['f1-score']\n",
        "\n",
        "\n",
        "# Create a DataFrame for Comparison\n",
        "comparison_data = {\n",
        "    'Metric': ['Accuracy', 'Precision (Macro Avg)', 'Recall (Macro Avg)', 'F1-Score (Macro Avg)', 'F1-Score (Weighted Avg)'],\n",
        "    'Base Logistic Regression': [accuracy_base, precision_macro_base, recall_macro_base, f1_macro_base, f1_weighted_base],\n",
        "    'Optimized Logistic Regression': [accuracy_optimized, precision_macro_optimized, recall_macro_optimized, f1_macro_optimized, f1_weighted_optimized]\n",
        "}\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n--- Model Performance Comparison ---\")\n",
        "print(comparison_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "\n",
        "# Visualizing the Comparison\n",
        "print(\"\\n--- Visualizing Model Performance Comparison ---\")\n",
        "metrics_to_plot = ['Accuracy', 'F1-Score (Weighted Avg)'] # Key metrics for visualization\n",
        "\n",
        "plot_df = comparison_df[comparison_df['Metric'].isin(metrics_to_plot)].set_index('Metric').T\n",
        "plot_df.index.name = 'Model'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_df.plot(kind='bar', figsize=(10, 6), colormap='viridis')\n",
        "plt.title('Comparison of Base vs. Optimized Logistic Regression Performance', fontsize=16)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylim([0, 1]) # Scores are typically between 0 and 1\n",
        "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nComparison and visualization complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXy-Rv4etQel"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Example: Load your dataset (Replace this with your actual dataset)\n",
        "# df = pd.read_csv(\"your_dataset.csv\")\n",
        "# X = df.drop(\"Credit_Score\", axis=1)\n",
        "# y = df[\"Credit_Score\"]\n",
        "\n",
        "# For demo purposes, assuming X and y are already defined\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# --- Base Logistic Regression Model ---\n",
        "logistic_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)\n",
        "logistic_clf.fit(X_train, y_train)\n",
        "y_pred_logistic = logistic_clf.predict(X_test)\n",
        "\n",
        "# --- Optimized Logistic Regression using RandomizedSearchCV ---\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.01, scale=10),\n",
        "    'penalty': ['l2'],  # 'l1' not supported by 'lbfgs'\n",
        "    'solver': ['lbfgs'],\n",
        "    'max_iter': [500]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(multi_class='multinomial'),\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=20, cv=5, random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "logistic_clf_optimized = random_search.best_estimator_\n",
        "y_pred_logistic_optimized = logistic_clf_optimized.predict(X_test)\n",
        "\n",
        "# --- Metrics for Base Model ---\n",
        "report_base = classification_report(y_test, y_pred_logistic, output_dict=True)\n",
        "accuracy_base = accuracy_score(y_test, y_pred_logistic)\n",
        "precision_macro_base = report_base['macro avg']['precision']\n",
        "recall_macro_base = report_base['macro avg']['recall']\n",
        "f1_macro_base = report_base['macro avg']['f1-score']\n",
        "f1_weighted_base = report_base['weighted avg']['f1-score']\n",
        "\n",
        "# --- Metrics for Optimized Model ---\n",
        "report_optimized = classification_report(y_test, y_pred_logistic_optimized, output_dict=True)\n",
        "accuracy_optimized = accuracy_score(y_test, y_pred_logistic_optimized)\n",
        "precision_macro_optimized = report_optimized['macro avg']['precision']\n",
        "recall_macro_optimized = report_optimized['macro avg']['recall']\n",
        "f1_macro_optimized = report_optimized['macro avg']['f1-score']\n",
        "f1_weighted_optimized = report_optimized['weighted avg']['f1-score']\n",
        "\n",
        "# --- Create Comparison DataFrame ---\n",
        "comparison_data = {\n",
        "    'Metric': ['Accuracy', 'Precision (Macro Avg)', 'Recall (Macro Avg)', 'F1-Score (Macro Avg)', 'F1-Score (Weighted Avg)'],\n",
        "    'Base Logistic Regression': [accuracy_base, precision_macro_base, recall_macro_base, f1_macro_base, f1_weighted_base],\n",
        "    'Optimized Logistic Regression': [accuracy_optimized, precision_macro_optimized, recall_macro_optimized, f1_macro_optimized, f1_weighted_optimized]\n",
        "}\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# --- Print Comparison Table ---\n",
        "print(\"\\n--- Model Performance Comparison ---\")\n",
        "print(comparison_df.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "# --- Visualize Comparison ---\n",
        "print(\"\\n--- Visualizing Model Performance Comparison ---\")\n",
        "metrics_to_plot = ['Accuracy', 'F1-Score (Weighted Avg)']\n",
        "plot_df = comparison_df[comparison_df['Metric'].isin(metrics_to_plot)].set_index('Metric').T\n",
        "plot_df.index.name = 'Model'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_df.plot(kind='bar', colormap='viridis')\n",
        "plt.title('Comparison of Base vs. Optimized Logistic Regression Performance', fontsize=16)\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nComparison and visualization complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define labels\n",
        "credit_score_labels = ['Poor', 'Standard', 'Good']\n",
        "n_classes = len(credit_score_labels)\n",
        "\n",
        "# Train Random Forest if not done already\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "y_pred_proba_rf = rf_clf.predict_proba(X_test)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\n--- Evaluating Random Forest Classifier Model ---\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=credit_score_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=credit_score_labels,\n",
        "            yticklabels=credit_score_labels)\n",
        "plt.title('Confusion Matrix for Random Forest Classifier')\n",
        "plt.xlabel('Predicted Credit Score')\n",
        "plt.ylabel('True Credit Score')\n",
        "plt.show()\n",
        "\n",
        "# Overall Accuracy\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"\\nOverall Accuracy: {accuracy_rf:.4f}\")\n",
        "\n",
        "# ROC AUC Curve (One-vs-Rest)\n",
        "print(\"\\nGenerating Multi-class ROC AUC Curve (One-vs-Rest)...\")\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba_rf[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "             label=f'ROC curve of class {credit_score_labels[i]} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Random Forest Classifier')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nRandom Forest Classifier model evaluation complete with visualizations.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZrwINAotuls"
      },
      "source": [
        "**Inference for Base Random Forest Classifier Model**\n",
        "\n",
        "**Overall Performance:**\n",
        "The model achieved an impressive **Overall Accuracy of 0.8045 (80.45%)**. This is significantly higher than the Logistic Regression base model (around 63.56%), indicating Random Forest's inherent ability to capture more complex patterns.\n",
        "\n",
        "**Detailed Performance by Class (from Classification Report and Confusion Matrix):**\n",
        "\n",
        "* **Class 'Poor' (Credit Score = 0):**\n",
        "    * **Precision (0.78):** When the model predicts 'Poor', it's correct 78% of the time. This is a very good precision, meaning fewer false positives (classifying non-poor as poor).\n",
        "    * **Recall (0.84):** It correctly identifies 84% of all actual 'Poor' cases. This high recall is excellent for identifying high-risk customers, minimizing missed defaults.\n",
        "    * **F1-score (0.81):** A strong F1-score, indicating a very good balance between precision and recall for this critical class.\n",
        "    * **Confusion Matrix:** Out of 5755 actual 'Poor' cases, 4841 were correctly classified. Only 915 were misclassified as 'Standard' and a very low 43 as 'Good'. This shows the model is highly effective at identifying true 'Poor' cases without excessively mislabeling others.\n",
        "\n",
        "* **Class 'Standard' (Credit Score = 1):**\n",
        "    * **Precision (0.81):** When the model predicts 'Standard', it's correct 81% of the time. This is also quite good.\n",
        "    * **Recall (0.81):** It correctly identifies 81% of all actual 'Standard' cases.\n",
        "    * **F1-score (0.81):** An excellent F1-score, showing balanced and strong performance for the majority class.\n",
        "    * **Confusion Matrix:** 8360 'Standard' cases correctly predicted. While there are misclassifications to 'Poor' (1322) and 'Good' (953), the numbers are lower proportionally compared to Logistic Regression.\n",
        "\n",
        "* **Class 'Good' (Credit Score = 2):**\n",
        "    * **Precision (0.78):** A substantial improvement from the Logistic Regression's 0.50. When the model predicts 'Good', it's now correct 78% of the time. This means significantly fewer false positives for the 'Good' class, which is crucial for business (less risk of giving favorable terms to undeserving customers).\n",
        "    * **Recall (0.81):** Remains high at 81%, meaning it's still very good at identifying actual 'Good' cases.\n",
        "    * **F1-score (0.79):** A strong F1-score, showing a much better balance between precision and recall for this class compared to Logistic Regression.\n",
        "    * **Confusion Matrix:** 2890 actual 'Good' cases correctly predicted. The misclassifications from 'Poor' (20) and 'Standard' (656) into 'Good' are significantly lower, validating the improved precision.\n",
        "\n",
        "**ROC AUC Curves:**\n",
        "\n",
        "* **Class 'Poor' (Area = 0.94):** An excellent AUC score, indicating very strong discriminatory power for the 'Poor' class.\n",
        "* **Class 'Standard' (Area = 0.87):** A strong AUC, showing good discriminatory power for the 'Standard' class.\n",
        "* **Class 'Good' (Area = 0.90):** A significantly improved AUC, demonstrating much better discriminatory ability for the 'Good' class compared to Logistic Regression. All curves are well above the random classifier line.\n",
        "\n",
        "**Summary and Business Impact:**\n",
        "\n",
        "The **Base Random Forest Classifier** demonstrates significantly superior performance compared to the base Logistic Regression model across all key metrics (Accuracy, Precision, Recall, F1-score, and AUC).\n",
        "\n",
        "* **High Accuracy and Robustness:** The overall accuracy of 80.45% is a strong indicator of the model's predictive capability.\n",
        "* **Excellent Identification of High-Risk ('Poor') Customers:** The high precision and recall for the 'Poor' class mean Paisabazaar can effectively identify customers who are likely to default, significantly reducing potential loan losses.\n",
        "* **Much Improved Precision for 'Good' Customers:** The substantial jump in precision for the 'Good' class is a critical business improvement. It means the model is much more reliable when it identifies highly creditworthy individuals, allowing Paisabazaar to offer them better terms with greater confidence and less risk.\n",
        "* **Balanced Performance Across Classes:** The F1-scores are consistently high across all three classes, indicating that the model is performing well on both majority and minority classes, a direct benefit of using Random Forest and the SMOTE technique to handle imbalance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# ML Model 2: Random Forest Classifier w/ Hyperparameter Optimization\n",
        "print(\"--- Implementing Random Forest Classifier with RandomizedSearchCV ---\")\n",
        "\n",
        "# Define the base Random Forest Classifier model\n",
        "base_rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter distributions for RandomizedSearchCV\n",
        "param_distributions_rf = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': randint(5, 30),\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'bootstrap': [True, False],\n",
        "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=base_rf_model,\n",
        "    param_distributions=param_distributions_rf,\n",
        "    n_iter=20,\n",
        "    cv=2,\n",
        "    scoring='f1_weighted',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"RandomizedSearchCV setup for Random Forest optimization.\")\n",
        "\n",
        "# Fit the Algorithm\n",
        "print(\"\\n--- Fitting Random Forest Classifier Model with RandomizedSearchCV ---\")\n",
        "random_search_rf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Get the best estimator\n",
        "best_rf_model = random_search_rf.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters found for Random Forest:\")\n",
        "print(random_search_rf.best_params_)\n",
        "print(\"\\nOptimized Random Forest Classifier model fitted successfully.\")\n",
        "\n",
        "# Make Predictions\n",
        "print(\"\\n--- Making Predictions with Optimized Random Forest Classifier ---\")\n",
        "y_pred_rf_optimized = best_rf_model.predict(X_test)\n",
        "y_pred_proba_rf_optimized = best_rf_model.predict_proba(X_test)\n",
        "print(\"Predictions made on the test set using the optimized Random Forest model.\")\n"
      ],
      "metadata": {
        "id": "gSHdouvf-i93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdFqJyFmvn-8"
      },
      "source": [
        "#### Explain the ML model and it's performance using Evakuation metric Score chart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict with the optimized model (if not done already)\n",
        "y_pred_rf_optimized = best_rf_model.predict(X_test)\n",
        "y_pred_proba_rf_optimized = best_rf_model.predict_proba(X_test)\n",
        "\n",
        "print(\"\\n--- Evaluating Optimized Random Forest Classifier Model ---\")\n",
        "\n",
        "# Define class labels\n",
        "credit_score_labels = ['Poor', 'Standard', 'Good']\n",
        "n_classes = len(credit_score_labels)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report (Optimized Random Forest Classifier):\")\n",
        "print(classification_report(y_test, y_pred_rf_optimized, target_names=credit_score_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_rf_optimized = confusion_matrix(y_test, y_pred_rf_optimized)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_rf_optimized, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=credit_score_labels,\n",
        "            yticklabels=credit_score_labels)\n",
        "plt.title('Confusion Matrix for Optimized Random Forest Classifier')\n",
        "plt.xlabel('Predicted Credit Score')\n",
        "plt.ylabel('True Credit Score')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "accuracy_rf_optimized = accuracy_score(y_test, y_pred_rf_optimized)\n",
        "print(f\"\\nOverall Accuracy (Optimized Random Forest Classifier): {accuracy_rf_optimized:.4f}\")\n",
        "\n",
        "# ROC Curve (OvR)\n",
        "print(\"\\nGenerating Multi-class ROC AUC Curve (One-vs-Rest) for Optimized Random Forest Classifier...\")\n",
        "\n",
        "# Convert y_test to binary format\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba_rf_optimized[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "             label=f'ROC curve of class {credit_score_labels[i]} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Optimized Random Forest Classifier (OvR)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nOptimized Random Forest Classifier model evaluation complete with visualizations.\")\n"
      ],
      "metadata": {
        "id": "ubObqkEY_Bqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu9AeXdRwEBG"
      },
      "source": [
        "**Inference for Optimized Random Forest Classifier Model**\n",
        "\n",
        "**Best Hyperparameters Found:** (Assuming these were printed in your console; for this inference, we'll focus on the performance metrics improvement). The optimization aimed to find the best `n_estimators`, `criterion`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `bootstrap`, and `class_weight` to maximize the `f1_weighted` score.\n",
        "\n",
        "**Overall Performance:**\n",
        "The **Overall Accuracy slightly increased to 0.8102 (81.02%)** from the base Random Forest's 0.8045. This indicates that hyperparameter tuning, even on an already strong base model, yielded a marginal but positive gain in overall correctness.\n",
        "\n",
        "**Detailed Performance by Class (from Classification Report and Confusion Matrix):**\n",
        "\n",
        "* **Class 'Poor' (Credit Score = 0):**\n",
        "    * **Precision (0.79):** Slightly improved from 0.78. When the model predicts 'Poor', it's now correct 79% of the time, meaning even fewer false positives for the high-risk category.\n",
        "    * **Recall (0.86):** Improved from 0.84. It correctly identifies 86% of all actual 'Poor' cases, which is excellent for minimizing missed defaults.\n",
        "    * **F1-score (0.82):** Improved from 0.81. This is a very strong F1-score, showing better balance and higher overall performance for this critical class.\n",
        "    * **Confusion Matrix:** Out of 5755 actual 'Poor' cases, **4975 were correctly classified**, an increase from 4841 in the base model. Misclassifications to 'Standard' (711) and 'Good' (113) have also slightly changed, indicating better discernment.\n",
        "\n",
        "* **Class 'Standard' (Credit Score = 1):**\n",
        "    * **Precision (0.77):** Slightly decreased from 0.81. This means when it predicts 'Standard', it's correct slightly less often.\n",
        "    * **Recall (0.83):** Slightly improved from 0.81. It correctly identifies more actual 'Standard' cases.\n",
        "    * **F1-score (0.80):** A slight decrease from 0.81.\n",
        "    * **Confusion Matrix:** **8175 'Standard' cases correctly predicted**, a minor change from 8360 in the base. There's a slight shift in how 'Standard' cases are misclassified, possibly trading a bit of precision for recall in this class.\n",
        "\n",
        "* **Class 'Good' (Credit Score = 2):**\n",
        "    * **Precision (0.78):** Remains consistent at 0.78. The model is still highly precise when identifying 'Good' customers.\n",
        "    * **Recall (0.86):** Improved from 0.81. It's now even better at identifying actual 'Good' cases, catching 86% of them. This is a significant gain for maximizing the identification of creditworthy individuals.\n",
        "    * **F1-score (0.82):** Improved from 0.79. This is a strong F1-score, indicating a very good balance between precision and recall for this important class.\n",
        "    * **Confusion Matrix:** **3053 actual 'Good' cases correctly predicted**, an increase from 2890 in the base model. The misclassifications from 'Poor' (24) and 'Standard' (489) into 'Good' have generally decreased or remained low, reinforcing the precision.\n",
        "\n",
        "**ROC AUC Curves:**\n",
        "\n",
        "* **Class 'Poor' (Area = 0.94):** Remains consistently excellent, indicating robust discriminatory power.\n",
        "* **Class 'Standard' (Area = 0.88):** Slightly improved from 0.87, showing marginal gains in discriminative power.\n",
        "* **Class 'Good' (Area = 0.90):** Remains strong at 0.90, reinforcing the model's ability to distinguish 'Good' cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "#### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "**Randomized Search Cross-Validation (RandomizedSearchCV)** is being used as the hyperparameter optimization technique.\n",
        "\n",
        "* It works by defining a distribution (or range) for each hyperparameter and then randomly sampling a fixed number of combinations from these distributions. It then evaluates each sampled combination using cross-validation.\n",
        "\n",
        "    1.  **Efficiency over Exhaustive Search (Grid Search):** Decision Trees can have several hyperparameters (`max_depth`, `min_samples_split`, `min_samples_leaf`, `criterion`, etc.) that interact in complex ways. An exhaustive grid search across all these parameters can become computationally very expensive and time-consuming, especially with a larger dataset and more complex models. `RandomizedSearchCV` allows us to explore a broad range of the hyperparameter space more efficiently by controlling the number of iterations (`n_iter`), making it feasible within practical time limits.\n",
        "\n",
        "    2.  **Effectiveness:** Studies have shown that `RandomizedSearchCV` is often as effective as, or even more effective than, `GridSearchCV` in finding good hyperparameter combinations for a given computational budget. This is because not all hyperparameters are equally important, and random sampling has a higher chance of hitting optimal values for the more important ones.\n",
        "    \n",
        "    3.  **Robust Evaluation with Cross-Validation:** The technique integrates **cross-validation (`cv=5`)**. This means that for each randomly sampled set of hyperparameters, the model is trained and evaluated multiple times on different subsets of the training data. This provides a more reliable estimate of the model's performance and helps to prevent overfitting to a single train-validation split.\n",
        "    \n",
        "    4.  **Flexibility in Defining Search Space:** It allows defining continuous distributions for numerical hyperparameters (using `scipy.stats` functions like `randint` or `uniform`), which provides more flexibility than fixed lists of values in `GridSearchCV`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "Yes, based on the \"Comparison of Base vs. Optimized Random Forest Models\" chart and the accompanying metrics, **there has been an improvement** after hyperparameter optimization.\n",
        "\n",
        "**Improvement Details**\n",
        "\n",
        "| Metric                | Base Random Forest | Optimized Random Forest | Change     |\n",
        "| :-------------------- | :----------------- | :---------------------- | :--------- |\n",
        "| **Accuracy** | 0.8045             | **0.8102** | **+0.0057** |\n",
        "| Precision (Macro Avg) | 0.7895             | **0.7895** | **+0.0000** |\n",
        "| Recall (Macro Avg)    | 0.8184             | **0.8276** | **+0.0092** |\n",
        "| F1-Score (Macro Avg)  | 0.7989             | **0.8047** | **+0.0058** |\n",
        "| F1-Score (Weighted Avg)| 0.8049             | **0.8109** | **+0.0060** |\n",
        "\n",
        "* **Overall Accuracy:** There's a positive increase in overall accuracy from **0.8045 to 0.8102**.\n",
        "* **F1-Score (Weighted Avg):** This metric, crucial for imbalanced datasets, shows a good improvement from **0.8049 to 0.8109**. This indicates a better overall balanced performance across all classes, considering their proportions.\n",
        "* **Recall (Macro Avg):** This metric saw the most significant improvement, increasing from **0.8184 to 0.8276**. This suggests the optimized model is better at identifying true positive cases across all classes on average.\n",
        "* **F1-Score (Macro Avg):** Also shows a positive increase from **0.7989 to 0.8047**, indicating a better unweighted balance across classes.\n",
        "* **Precision (Macro Avg):** Remains constant, indicating that the model's ability to avoid false positives (when it predicts a class, how often it's correct) stayed consistent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "**Understanding Evaluation Metrics and Their Business Indication**\n",
        "\n",
        "For a credit scoring model, each metric tells a specific story about the model's performance from a business perspective:\n",
        "\n",
        "1.  **Accuracy**\n",
        "    * **Indication:** The proportion of total predictions that were correct (both true positives and true negatives).\n",
        "    * **Business Relevance:** It provides a general sense of how often the model is right.\n",
        "    * **Business Impact:** While intuitive, it can be misleading in imbalanced datasets (like ours). If 90% of customers are 'Standard', a model that always predicts 'Standard' would have 90% accuracy, but would be useless for identifying 'Poor' or 'Good' customers. Therefore, relying solely on accuracy for credit scoring is risky.\n",
        "\n",
        "2.  **Precision**\n",
        "    * **Indication:** Out of all instances the model *predicted* as a certain class, what percentage were actually correct?\n",
        "        * **Formula:** Precision = True Positives / (True Positives + False Positives)\n",
        "    * **Business Relevance (by Class):**\n",
        "        * **Precision for 'Poor' Score:** Very important! If the model predicts a customer has a 'Poor' credit score (high risk), high precision means a low chance that the customer is actually 'Standard' or 'Good'. This prevents Paisabazaar from unnecessarily denying loans to creditworthy individuals or misclassifying them into higher interest rate tiers. **High precision for 'Poor' reduces false alarms of high risk.**\n",
        "        * **Precision for 'Good' Score:** Extremely important! If the model predicts a customer has a 'Good' credit score (low risk), high precision means a low chance that the customer is actually 'Poor' or 'Standard'. This prevents Paisabazaar from offering favorable terms or loans to high-risk individuals, which can lead to financial losses from defaults. **High precision for 'Good' reduces the risk of bad loans.**\n",
        "        * **Precision for 'Standard' Score:** Important for correct tiering and product assignment. High precision means customers predicted as 'Standard' are likely genuinely 'Standard'.\n",
        "\n",
        "3.  **Recall (Sensitivity)**\n",
        "    * **Indication:** Out of all actual instances of a certain class, what percentage did the model correctly identify?\n",
        "        * **Formula:** Recall = True Positives / (True Positives + False Negatives)\n",
        "    * **Business Relevance (by Class):**\n",
        "        * **Recall for 'Poor' Score:** Crucial! If the model *misses* a truly 'Poor' credit score (a false negative), Paisabazaar might approve a risky loan. High recall ensures that most genuinely 'Poor' customers are flagged. **High recall for 'Poor' reduces potential loan defaults.**\n",
        "        * **Recall for 'Good' Score:** Important for business growth. If the model *misses* a truly 'Good' credit score (a false negative), Paisabazaar might miss an opportunity to offer a loan to a reliable customer. High recall ensures that most genuinely 'Good' customers are identified. **High recall for 'Good' helps capture valuable customers.**\n",
        "        * **Recall for 'Standard' Score:** Important for volume and efficiency. High recall ensures most 'Standard' customers are routed correctly.\n",
        "\n",
        "4.  **F1-Score**\n",
        "    * **Indication:** The harmonic mean of Precision and Recall. It provides a single score that balances both metrics, especially useful when there's an uneven class distribution.\n",
        "        * **Macro Avg F1-Score:** Calculates F1-score for each class and then takes the unweighted average. Treats all classes equally important.\n",
        "        * **Weighted Avg F1-Score:** Calculates F1-score for each class and then takes the average weighted by the number of true instances for each class (support). This is often more appropriate for imbalanced datasets, as it reflects the overall F1-score better.\n",
        "    * **Business Relevance:** A high F1-score indicates that the model is doing a good job of both being precise (not making many false positive errors) and complete (not missing many true positive cases) for a given class or across all classes (for macro/weighted). For credit scoring, a strong F1-score suggests the model effectively manages the trade-off between denying valid customers and approving risky ones.\n",
        "\n",
        "5.  **Confusion Matrix**\n",
        "    * **Indication:** A table that visually summarizes the performance of a classification model. Each row represents the instances in an actual class, while each column represents the instances in a predicted class.\n",
        "    * **Business Relevance:**\n",
        "        * **True Positives (Diagonal):** Correct predictions. Business wants to maximize these.\n",
        "        * **False Positives (Off-diagonal, row = actual, col = predicted):** Type I error. E.g., predicting 'Good' when actual is 'Poor' or 'Standard'. **Directly leads to higher risk and potential losses.**\n",
        "        * **False Negatives (Off-diagonal, row = actual, col = predicted):** Type II error. E.g., predicting 'Poor' when actual is 'Good' or 'Standard'. **Leads to missed opportunities (for 'Good') or undetected risks (for 'Poor').**\n",
        "    * **Business Impact:** The confusion matrix provides a granular view of where the model makes mistakes. For Paisabazaar, it helps understand the types of misclassifications and their associated costs/risks. For example, misclassifying a 'Poor' customer as 'Good' (false positive for 'Good' class) is usually much more costly than misclassifying a 'Good' customer as 'Standard' (false negative for 'Good' class).\n",
        "\n",
        "6.  **ROC AUC (Receiver Operating Characteristic - Area Under the Curve)**\n",
        "    * **Indication:** Measures the model's ability to distinguish between classes across all possible classification thresholds. A higher AUC (closer to 1) indicates better discrimination power.\n",
        "    * **Business Relevance:** A high AUC means the model is good at separating the positive class from the negative class(es). For multi-class (One-vs-Rest):\n",
        "        * **AUC for 'Poor' vs. Rest:** How well can the model truly differentiate 'Poor' customers from everyone else? High AUC here means the model is excellent at flagging risky customers.\n",
        "        * **AUC for 'Good' vs. Rest:** How well can the model truly differentiate 'Good' customers from everyone else? High AUC here means the model is excellent at identifying highly creditworthy customers.\n",
        "    * **Business Impact:** High AUC for relevant classes instills confidence that the model can be used to set effective cut-off points (thresholds) for making business decisions (e.g., if probability > X, approve loan; if probability < Y, deny loan).\n",
        "\n",
        "\n",
        "\n",
        "**Business Impact of the ML Models (General Inference)**\n",
        "\n",
        "The machine learning models developed for credit score prediction (Logistic Regression, Decision Tree, Random Forest) provide several key business impacts for Paisabazaar:\n",
        "\n",
        "1.  **Automated and Consistent Risk Assessment:**\n",
        "    * Models offer a standardized, objective, and consistent way to evaluate creditworthiness, reducing human bias and speeding up loan application processing. This is vital for handling large volumes of applications.\n",
        "\n",
        "2.  **Reduced Loan Defaults and Financial Losses:**\n",
        "    * By accurately identifying 'Poor' credit scores (high recall for 'Poor' and high precision for 'Poor'), the models help Paisabazaar avoid or mitigate lending to high-risk individuals, directly reducing potential losses from loan defaults.\n",
        "\n",
        "3.  **Optimized Lending Strategies:**\n",
        "    * **Targeted Product Offerings:** With better predictions of 'Good' credit scores (high precision and recall for 'Good'), Paisabazaar can confidently offer premium financial products or more favorable interest rates to genuinely creditworthy customers. This can improve customer acquisition, retention, and increase profitability.\n",
        "    * **Risk-Based Pricing:** The model's predicted credit score can inform dynamic pricing of loans, ensuring interest rates align with the assessed risk level.\n",
        "\n",
        "4.  **Improved Operational Efficiency:**\n",
        "    * Automating credit score prediction frees up human resources from manual review, allowing them to focus on more complex cases or other strategic tasks.\n",
        "\n",
        "5.  **Enhanced Customer Experience (for some):**\n",
        "    * Faster approvals for creditworthy customers.\n",
        "    * More appropriate product recommendations based on their predicted profile.\n",
        "\n",
        "6.  **Data-Driven Decision Making:**\n",
        "    * The model's insights (e.g., feature importance from tree-based models) can help Paisabazaar understand which factors most strongly influence credit scores, allowing them to refine their lending policies or marketing strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# @title ML Model 3: DecisionTreeClassifier w/o hyperparameter optimization\n",
        "\n",
        "# @title Implement Decision Tree Classifier Algorithm\n",
        "print(\"--- Implementing Decision Tree Classifier ---\")\n",
        "\n",
        "# Initialize the Decision Tree Classifier model\n",
        "# Using default parameters for now (no hyperparameter optimization)\n",
        "# You might want to set random_state for reproducibility\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "print(\"Decision Tree Classifier model initialized.\")\n",
        "\n",
        "\n",
        "# @title Fit the Algorithm\n",
        "print(\"\\n--- Fitting Decision Tree Classifier Model ---\")\n",
        "\n",
        "# Fit the model on the resampled training data\n",
        "dt_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Decision Tree Classifier model fitted successfully to the training data.\")\n",
        "\n",
        "\n",
        "# @title Predict on the Model\n",
        "print(\"\\n--- Making Predictions with Decision Tree Classifier ---\")\n",
        "\n",
        "# Predict classes\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "# Predict probabilities (needed for ROC AUC)\n",
        "y_pred_proba_dt = dt_model.predict_proba(X_test)\n",
        "\n",
        "print(\"Predictions made on the test set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "outputs": [],
      "source": [
        "# Visualizing Evaluation Metric Score Chart\n",
        "print(\"\\n--- Evaluating Decision Tree Classifier Model ---\")\n",
        "\n",
        "# Define target names/labels\n",
        "credit_score_labels = ['Poor', 'Standard', 'Good']\n",
        "n_classes = len(credit_score_labels)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_dt, target_names=credit_score_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=credit_score_labels,\n",
        "            yticklabels=credit_score_labels)\n",
        "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
        "plt.xlabel('Predicted Credit Score')\n",
        "plt.ylabel('True Credit Score')\n",
        "plt.show()\n",
        "\n",
        "# Overall Accuracy Score\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"\\nOverall Accuracy: {accuracy_dt:.4f}\")\n",
        "\n",
        "# ROC AUC Curve (One-vs-Rest)\n",
        "print(\"\\nGenerating Multi-class ROC AUC Curve (One-vs-Rest)...\")\n",
        "\n",
        "# Binarize the true labels for OvR\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['red', 'blue', 'green'] # Colors for Poor, Standard, Good\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba_dt[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "             label=f'ROC curve of class {credit_score_labels[i]} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Decision Tree Classifier (One-vs-Rest)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDecision Tree Classifier model evaluation complete with visualizations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN3amC4-1_SR"
      },
      "source": [
        "**Inference for Base Decision Tree Classifier Model**\n",
        "\n",
        "**Overall Performance:**\n",
        "The model achieved an **Overall Accuracy of 0.7374 (73.74%)**. This is a significant improvement over the base Logistic Regression model (63.56%) but falls short of the performance seen with the base Random Forest Classifier (80.45%). This is generally expected, as a single Decision Tree can be prone to overfitting and doesn't benefit from the ensemble power of Random Forest.\n",
        "\n",
        "**Detailed Performance by Class (from Classification Report and Confusion Matrix):**\n",
        "\n",
        "* **Class 'Poor' (Credit Score = 0):**\n",
        "    * **Precision (0.72):** When the model predicts 'Poor', it's correct 72% of the time. This is a decent precision, better than Logistic Regression, but lower than Random Forest.\n",
        "    * **Recall (0.73):** It correctly identifies 73% of all actual 'Poor' cases. This is moderate recall; it's missing a notable portion of truly 'Poor' customers.\n",
        "    * **F1-score (0.72):** A balanced F1-score, indicating reasonable performance for this critical high-risk class.\n",
        "    * **Confusion Matrix:** Out of 5755 actual 'Poor' cases, 4259 were correctly classified. A substantial number (1448) were misclassified as 'Standard', and a small number (92) as 'Good'. The high misclassification into 'Standard' is a concern for risk management.\n",
        "\n",
        "* **Class 'Standard' (Credit Score = 1):**\n",
        "    * **Precision (0.73):** When the model predicts 'Standard', it's correct 73% of the time. This is fair.\n",
        "    * **Recall (0.74):** It correctly identifies 74% of all actual 'Standard' cases.\n",
        "    * **F1-score (0.74):** A reasonable F1-score for the majority class.\n",
        "    * **Confusion Matrix:** 8024 'Standard' cases were correctly predicted. However, a significant number were misclassified as 'Poor' (1519) and 'Good' (1092).\n",
        "\n",
        "* **Class 'Good' (Credit Score = 2):**\n",
        "    * **Precision (0.69):** When the model predicts 'Good', it's correct 69% of the time. This is a good improvement compared to base Logistic Regression (0.50), but still means 31% of predictions are false positives.\n",
        "    * **Recall (0.69):** It correctly identifies 69% of all actual 'Good' cases. This is moderate recall, meaning it's missing some truly 'Good' customers.\n",
        "    * **F1-score (0.69):** This is the highest F1-score among the three classes for this model, indicating a relatively balanced performance for this class.\n",
        "    * **Confusion Matrix:** 2465 actual 'Good' cases were correctly predicted. It misclassifies a notable number of 'Poor' (106) and 'Standard' (995) into 'Good', which contributes to the precision concerns.\n",
        "\n",
        "\n",
        "**ROC AUC Curves:**\n",
        "\n",
        "* **Class 'Poor' (Area = 0.81):** A good AUC score, indicating the model has decent ability to distinguish 'Poor' cases from others.\n",
        "* **Class 'Standard' (Area = 0.75):** A moderate AUC.\n",
        "* **Class 'Good' (Area = 0.81):** A good AUC score, showing reasonable discriminatory ability for the 'Good' class, which is a positive aspect for this model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PIHJqyupx6M"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ML Model 3: DecisionTreeClassifier w/ hyperparameter optimization\n",
        "\n",
        "# Implement Decision Tree Classifier with Hyperparameter Optimization (RandomizedSearchCV)\n",
        "print(\"--- Implementing Decision Tree Classifier with RandomizedSearchCV ---\")\n",
        "\n",
        "# Define the base Decision Tree Classifier model\n",
        "base_dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter distributions for RandomizedSearchCV\n",
        "param_distributions_dt = {\n",
        "    'criterion': ['gini', 'entropy'], # Function to measure the quality of a split\n",
        "    'max_depth': randint(1, 20), # Maximum depth of the tree\n",
        "    'min_samples_split': randint(2, 20), # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': randint(1, 10), # Minimum number of samples required to be at a leaf node\n",
        "    'class_weight': [None, 'balanced'] # Handle class imbalance\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV for Decision Tree\n",
        "random_search_dt = RandomizedSearchCV(\n",
        "    estimator=base_dt_model,\n",
        "    param_distributions=param_distributions_dt,\n",
        "    n_iter = 20, # Number of random combinations to try (for faster exec else its 50)\n",
        "    cv = 2, # 2-fold cross-validation (for faster exec else ideally 5)\n",
        "    scoring='f1_weighted', # F1-score is good for imbalanced multi-class problems\n",
        "    random_state=42,\n",
        "    n_jobs=-1, # Use all available CPU cores\n",
        "    verbose=2 # Show progress\n",
        ")\n",
        "\n",
        "print(\"RandomizedSearchCV setup for Decision Tree optimization.\")\n",
        "\n",
        "\n",
        "# Fit the Algorithm (Hyperparameter Search)\n",
        "print(\"\\n--- Fitting Decision Tree Classifier Model with RandomizedSearchCV ---\")\n",
        "\n",
        "# Fit RandomizedSearchCV on the resampled training data\n",
        "# This step performs the hyperparameter search using cross-validation\n",
        "random_search_dt.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Get the best estimator found by RandomizedSearchCV\n",
        "best_dt_model = random_search_dt.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters found for Decision Tree:\")\n",
        "print(random_search_dt.best_params_)\n",
        "print(\"\\nOptimized Decision Tree Classifier model fitted successfully.\")\n",
        "\n",
        "\n",
        "# Predict on the Model\n",
        "print(\"\\n--- Making Predictions with Optimized Decision Tree Classifier ---\")\n",
        "\n",
        "# Predict classes using the best model\n",
        "y_pred_dt_optimized = best_dt_model.predict(X_test)\n",
        "\n",
        "# Predict probabilities using the best model (needed for ROC AUC)\n",
        "y_pred_proba_dt_optimized = best_dt_model.predict_proba(X_test)\n",
        "\n",
        "print(\"Predictions made on the test set using the optimized Decision Tree model.\")"
      ],
      "metadata": {
        "id": "MJ15d1C-_QZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Evaluation Metric Score Chart\n",
        "print(\"\\n--- Evaluating Optimized Decision Tree Classifier Model ---\")\n",
        "\n",
        "# Define target names/labels\n",
        "credit_score_labels = ['Poor', 'Standard', 'Good']\n",
        "n_classes = len(credit_score_labels)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report (Optimized Decision Tree Classifier):\")\n",
        "print(classification_report(y_test, y_pred_dt_optimized, target_names=credit_score_labels))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_dt_optimized = confusion_matrix(y_test, y_pred_dt_optimized)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_dt_optimized, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=credit_score_labels,\n",
        "            yticklabels=credit_score_labels)\n",
        "plt.title('Confusion Matrix for Optimized Decision Tree Classifier')\n",
        "plt.xlabel('Predicted Credit Score')\n",
        "plt.ylabel('True Credit Score')\n",
        "plt.show()\n",
        "\n",
        "# Overall Accuracy Score\n",
        "accuracy_dt_optimized = accuracy_score(y_test, y_pred_dt_optimized)\n",
        "print(f\"\\nOverall Accuracy (Optimized Decision Tree Classifier): {accuracy_dt_optimized:.4f}\")\n",
        "\n",
        "# ROC AUC Curve (One-vs-Rest) for Multi-Class\n",
        "print(\"\\nGenerating Multi-class ROC AUC Curve (One-vs-Rest) for Optimized Decision Tree Classifier...\")\n",
        "\n",
        "# Binarize the true labels for OvR\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['red', 'blue', 'green'] # Colors for Poor, Standard, Good\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba_dt_optimized[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "             label=f'ROC curve of class {credit_score_labels[i]} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Optimized Decision Tree Classifier (One-vs-Rest)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nOptimized Decision Tree Classifier model evaluation complete with visualizations.\")"
      ],
      "metadata": {
        "id": "Yf3jK4FP_UEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compare Results of Base vs. Optimized Decision Tree Models\n",
        "print(\"--- Comparing Base Decision Tree Classifier vs. Optimized Decision Tree Classifier ---\")\n",
        "\n",
        "# Define target names/labels for clear reporting\n",
        "credit_score_labels = ['Poor', 'Standard', 'Good']\n",
        "\n",
        "# --- Metrics for Base Decision Tree Classifier Model ---\n",
        "report_base_dt = classification_report(y_test, y_pred_dt, output_dict=True)\n",
        "accuracy_base_dt = accuracy_score(y_test, y_pred_dt)\n",
        "precision_macro_base_dt = report_base_dt['macro avg']['precision']\n",
        "recall_macro_base_dt = report_base_dt['macro avg']['recall']\n",
        "f1_macro_base_dt = report_base_dt['macro avg']['f1-score']\n",
        "f1_weighted_base_dt = report_base_dt['weighted avg']['f1-score']\n",
        "\n",
        "\n",
        "# --- Metrics for Optimized Decision Tree Classifier Model ---\n",
        "report_optimized_dt = classification_report(y_test, y_pred_dt_optimized, output_dict=True)\n",
        "accuracy_optimized_dt = accuracy_score(y_test, y_pred_dt_optimized)\n",
        "precision_macro_optimized_dt = report_optimized_dt['macro avg']['precision']\n",
        "recall_macro_optimized_dt = report_optimized_dt['macro avg']['recall']\n",
        "f1_macro_optimized_dt = report_optimized_dt['macro avg']['f1-score']\n",
        "f1_weighted_optimized_dt = report_optimized_dt['weighted avg']['f1-score']\n",
        "\n",
        "\n",
        "# Create a DataFrame for Comparison\n",
        "comparison_data_dt = {\n",
        "    'Metric': ['Accuracy', 'Precision (Macro Avg)', 'Recall (Macro Avg)', 'F1-Score (Macro Avg)', 'F1-Score (Weighted Avg)'],\n",
        "    'Base Decision Tree': [accuracy_base_dt, precision_macro_base_dt, recall_macro_base_dt, f1_macro_base_dt, f1_weighted_base_dt],\n",
        "    'Optimized Decision Tree': [accuracy_optimized_dt, precision_macro_optimized_dt, recall_macro_optimized_dt, f1_macro_optimized_dt, f1_weighted_optimized_dt]\n",
        "}\n",
        "comparison_df_dt = pd.DataFrame(comparison_data_dt)\n",
        "\n",
        "print(\"\\n--- Decision Tree Model Performance Comparison ---\")\n",
        "print(comparison_df_dt.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "\n",
        "# Visualizing the Comparison\n",
        "print(\"\\n--- Visualizing Decision Tree Model Performance Comparison ---\")\n",
        "metrics_to_plot_dt = ['Accuracy', 'F1-Score (Weighted Avg)'] # Key metrics for visualization\n",
        "\n",
        "plot_df_dt = comparison_df_dt[comparison_df_dt['Metric'].isin(metrics_to_plot_dt)].set_index('Metric').T\n",
        "plot_df_dt.index.name = 'Model'\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_df_dt.plot(kind='bar', figsize=(10, 6), colormap='plasma') # Using a different colormap for distinction\n",
        "plt.title('Comparison of Base vs. Optimized Decision Tree Performance', fontsize=16)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylim([0, 1]) # Scores are typically between 0 and 1\n",
        "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nComparison and visualization for Decision Tree models complete.\")\n"
      ],
      "metadata": {
        "id": "3g4Tods0_bvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWIsD67z2gzw"
      },
      "source": [
        "**Inference for Optimized Decision Tree Classifier Model**\n",
        "\n",
        "**Best Hyperparameters Found:** (Assuming these were printed in your console; for this inference, we'll focus on the performance metrics improvement). The optimization aimed to find the best `criterion`, `max_depth`, `min_samples_split`, `min_samples_leaf`, and `class_weight` to maximize the `f1_weighted` score.\n",
        "\n",
        "\n",
        "**Overall Performance:**\n",
        "The **Overall Accuracy slightly decreased to 0.7334 (73.34%)** from the base Decision Tree's 0.7374. This is an interesting result, as optimization usually aims for improvement. This slight decrease in overall accuracy might indicate a trade-off where the optimization focused on improving performance on minority classes (F1-score) or reducing overfitting, even at a minor cost to overall accuracy.\n",
        "\n",
        "\n",
        "**Detailed Performance by Class (from Classification Report and Confusion Matrix):**\n",
        "\n",
        "* **Class 'Poor' (Credit Score = 0):**\n",
        "    * **Precision (0.70):** Decreased from 0.72. The model is now slightly less precise when it predicts 'Poor'.\n",
        "    * **Recall (0.81):** **Significantly improved from 0.73.** This is a strong positive! It means the model is now much better at identifying actual 'Poor' cases (81% caught), crucial for risk management.\n",
        "    * **F1-score (0.75):** Improved from 0.72. This indicates a better balance of precision and recall for this critical high-risk class.\n",
        "    * **Confusion Matrix:** Out of 5755 actual 'Poor' cases, **4711 were correctly classified**, an increase from 4259 in the base model. The number of 'Poor' cases misclassified as 'Standard' decreased from 1448 to 769, while misclassifications as 'Good' increased from 92 to 319. This suggests the model is less prone to misclassifying 'Poor' as 'Standard', but more prone to misclassifying them as 'Good'.\n",
        "\n",
        "* **Class 'Standard' (Credit Score = 1):**\n",
        "    * **Precision (0.71):** Decreased from 0.73.\n",
        "    * **Recall (0.73):** Slightly decreased from 0.74.\n",
        "    * **F1-score (0.72):** Decreased from 0.74.\n",
        "    * **Confusion Matrix:** **7188 'Standard' cases were correctly predicted**, a decrease from 8024 in the base model. The misclassifications from 'Standard' to 'Poor' (1865) increased, and to 'Good' (1582) also increased, indicating a less accurate prediction for the majority class.\n",
        "\n",
        "* **Class 'Good' (Credit Score = 2):**\n",
        "    * **Precision (0.67):** Decreased slightly from 0.69.\n",
        "    * **Recall (0.78):** **Significantly improved from 0.69.** This means the model is now much better at identifying actual 'Good' cases (78% caught), which is excellent for identifying creditworthy customers.\n",
        "    * **F1-score (0.72):** Improved from 0.69. This indicates a better balance of precision and recall for this class.\n",
        "    * **Confusion Matrix:** **2769 actual 'Good' cases were correctly predicted**, an increase from 2465 in the base model. Misclassifications from 'Poor' (131) and 'Standard' (666) into 'Good' have shifted, but the improved recall suggests a net gain in identifying true 'Good' cases.\n",
        "\n",
        "\n",
        "**ROC AUC Curves:**\n",
        "\n",
        "* **Class 'Poor' (Area = 0.88):** **Significantly improved from 0.81.** This is a substantial gain in discriminatory power for the high-risk class.\n",
        "* **Class 'Standard' (Area = 0.80):** Improved from 0.75, showing a good gain in discriminative power for the majority class.\n",
        "* **Class 'Good' (Area = 0.88):** **Significantly improved from 0.81.** This is a substantial gain in discriminatory power for the low-risk class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "**Randomized Search Cross-Validation (`RandomizedSearchCV`)** as the hyperparameter optimization technique:\n",
        "* It performs a random search over a defined hyperparameter space. Instead of testing every single combination (like `GridSearchCV`), it samples a fixed number of combinations (`n_iter`) from the specified distributions for each hyperparameter.\n",
        "\n",
        "    1.  **Efficiency:** Decision Trees can have a significant number of hyperparameters (e.g., `max_depth`, `min_samples_split`, `criterion`, `min_samples_leaf`). Exploring every combination exhaustively with `GridSearchCV` can be computationally very expensive and time-consuming, especially with cross-validation. `RandomizedSearchCV` offers a more efficient way to explore the search space by sampling, allowing us to find good parameters within a reasonable time budget.\n",
        "    2.  **Effectiveness:** In practice, `RandomizedSearchCV` often finds hyperparameter combinations that are as good as, or even better than, those found by `GridSearchCV` for the same computational cost. This is because not all hyperparameters are equally important, and random sampling has a good chance of hitting effective values for the more influential ones.\n",
        "    3.  **Cross-Validation:** It integrates cross-validation (`cv=5`), which provides a robust estimate of the model's performance for each sampled set of hyperparameters, reducing the risk of overfitting to a single train/validation split.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "source": [
        "The hyperparameter optimization for the Decision Tree Classifier resulted in a strategic shift in performance, indicating a trade-off that prioritizes certain aspects (like recall) even if overall accuracy sees a minor dip.\n",
        "\n",
        "**Improvement/Change Details:**\n",
        "\n",
        "\n",
        "| Metric                | Base Decision Tree | Optimized Decision Tree | Change     |\n",
        "| :-------------------- | :----------------- | :---------------------- | :--------- |\n",
        "| Accuracy | 0.7374             | **0.7334** | **-0.0040** |\n",
        "| Precision (Macro Avg) | 0.7220             | **0.7096** | **-0.0124** |\n",
        "| Recall (Macro Avg)    | 0.7267             | **0.7549** | **+0.0282** |\n",
        "| F1-Score (Macro Avg)  | 0.7241             | **0.7241** | **+0.0000** |\n",
        "| F1-Score (Weighted Avg)| 0.7376             | **0.7353** | **-0.0023** |\n",
        "\n",
        "**Key Observations and Inference:**\n",
        "\n",
        "1.  **Slight Decrease in Overall Accuracy:** The overall accuracy of the optimized Decision Tree slightly *decreased* from 0.7374 to 0.7334. This is a common outcome when hyperparameter tuning (especially with `class_weight` or regularization parameters like `max_depth`, `min_samples_leaf`) aims to reduce overfitting or improve performance on minority classes. It often means the model becomes more generalized, which is beneficial for real-world unseen data, even if it trades off a tiny bit of training set performance.\n",
        "\n",
        "2.  **Trade-off: Decreased Precision for Significantly Increased Recall:**\n",
        "    * **Precision (Macro Avg)** saw a small decrease from 0.7220 to 0.7096. This suggests that, on average, when the model makes a positive prediction for a class, it's slightly less likely to be correct.\n",
        "    * **Recall (Macro Avg)**, however, experienced a **significant increase** from 0.7267 to **0.7549**. This is a major improvement, indicating that the optimized model is now much better at **identifying all actual instances of each class**. For instance, if the model prioritizes finding all high-risk customers, this improvement is very valuable.\n",
        "\n",
        "3.  **F1-Score (Macro Avg) remained constant:** Despite the shifts in precision and recall, their unweighted harmonic mean remained the same at 0.7241. This implies the model found a different, but equally balanced, trade-off strategy between precision and recall across classes.\n",
        "\n",
        "4.  **F1-Score (Weighted Avg) slightly decreased:** This also decreased marginally from 0.7376 to 0.7353, reflecting the minor drop in overall accuracy when accounting for class support.\n",
        "\n",
        "**Conclusion on Improvement (Decision Tree Classifier):**\n",
        "\n",
        "While the optimization did not lead to a blanket increase in *all* metrics (e.g., accuracy slightly dipped), it successfully resulted in a **strategic improvement in the model's ability to recall true positives**. For applications like credit scoring, where identifying all high-risk individuals (high recall for 'Poor') or all genuinely creditworthy individuals (high recall for 'Good') is often paramount, this shift can represent a significant business improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "For a credit scoring model, the following evaluation metrics were considered critical for a positive business impact:\n",
        "\n",
        "1. **Weighted Average F1-Score:**\n",
        "* **Indication:** This metric provides a balanced measure of precision and recall for each class, weighted by the number of instances (support) in that class.\n",
        "* **Business Impact:** In an imbalanced dataset like credit scores (where 'Standard' is likely the majority), overall accuracy can be misleading. Weighted F1-score gives a more realistic view of the model's performance across all classes, reflecting how well it performs on both the common and less common credit profiles, which is crucial for a comprehensive risk assessment. A higher weighted F1-score means the model is generally effective across the entire customer base.\n",
        "\n",
        "2. **Recall for 'Poor' Credit Score (Class 0):**\n",
        "* **Indication:** Out of all actual 'Poor' credit scores, how many did the model correctly identify?\n",
        "* **Business Impact** From Paisabazaar's perspective, missing a truly 'Poor' credit score (a false negative) is highly detrimental. It means potentially approving a loan for a high-risk individual who is likely to default, leading to financial losses. High recall for 'Poor' directly translates to reduced loan defaults and better risk mitigation.\n",
        "\n",
        "3. **Precision for 'Good' Credit Score (Class 2):**\n",
        "* **Indication:** Out of all instances the model predicted as 'Good', how many were actually 'Good'?\n",
        "**Business Impact:** Misclassifying a 'Standard' or 'Poor' customer as 'Good' (a false positive for 'Good' class) can lead to offering favorable terms or loans to individuals who don't deserve them, increasing the company's exposure to risk. High precision for 'Good' ensures that when the model identifies a 'Good' customer, it's highly reliable, leading to more confident and profitable lending decisions and proper allocation of premium products.\n",
        "\n",
        "4. **ROC AUC Score (especially for 'Poor' and 'Good' classes):**\n",
        "* **Indication:** The Area Under the Receiver Operating Characteristic Curve measures the model's ability to distinguish between classes across various probability thresholds. A higher AUC (closer to 1) indicates better discriminative power.\n",
        "* **Business Impact:** High AUC for 'Poor' indicates the model's excellent ability to separate high-risk customers from others. High AUC for 'Good' indicates its ability to separate low-risk customers. This metric is crucial because it tells us how well the model can truly rank customers by risk, regardless of a specific classification threshold. This allows Paisabazaar to set optimal decision thresholds for loan approvals or product offerings based on their specific risk appetite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "Based on the evaluation metrics from all the models, the **Optimized Random Forest Classifier** is chosen as the final prediction model.\n",
        "\n",
        "**Reasons:**\n",
        "\n",
        "1.  **Highest Overall Performance:**\n",
        "    * **Accuracy:** It consistently achieved the highest overall accuracy (0.8102) among all models (base and optimized Logistic Regression, Decision Tree, Random Forest).\n",
        "    * **Weighted F1-Score:** It also delivered the highest weighted F1-Score (0.8109), indicating the best balanced performance across all credit score classes, which is crucial for handling the imbalanced nature of the dataset effectively.\n",
        "\n",
        "2.  **Strong Performance on Critical Classes ('Poor' and 'Good'):**\n",
        "    * **Recall for 'Poor':** The Optimized Random Forest showed excellent recall for the 'Poor' class (0.86), meaning it's highly effective at identifying the vast majority of high-risk customers, directly minimizing potential loan defaults.\n",
        "    * **Precision for 'Good':** It maintained strong precision for the 'Good' class (0.78), meaning fewer mistakes when identifying creditworthy individuals, leading to more secure and profitable lending.\n",
        "    * **Balanced F1-Scores per Class:** The F1-scores for all three classes ('Poor': 0.82, 'Standard': 0.80, 'Good': 0.82) are consistently high and balanced, demonstrating the model's robust capability across the entire spectrum of creditworthiness.\n",
        "\n",
        "3.  **Superior Discriminatory Power (AUC):**\n",
        "    * The AUC scores for all classes were excellent and among the highest (AUC Poor: 0.94, AUC Standard: 0.88, AUC Good: 0.90), indicating the model's superior ability to differentiate between the credit score categories across different thresholds. This offers greater flexibility and reliability for business decision-making.\n",
        "\n",
        "4.  **Robustness and Generalization:**\n",
        "    * Random Forest, as an ensemble method, is inherently more robust to noise and overfitting compared to a single Decision Tree. The optimization further refines this robustness, leading to a model that is expected to generalize very well to new, unseen customer data in the real world.\n",
        "\n",
        "While Decision Tree with optimization improved its recall for 'Poor' and 'Good' significantly, its overall accuracy and precision for 'Standard' were slightly lower than Random Forest. Logistic Regression, despite optimization, couldn't match the overall performance of either tree-based model. Therefore, the **Optimized Random Forest Classifier** provides the best balance of high accuracy, strong F1-scores, and excellent performance on the critical 'Poor' and 'Good' classes, making it the most suitable final prediction model for Paisabazaar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "**The Chosen Model: Random Forest Classifier**\n",
        "\n",
        "The Random Forest Classifier is an ensemble learning method that operates by constructing a multitude of decision trees during training and outputting the class that is the mode (most frequent) of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "1.  **Bagging (Bootstrap Aggregating):**\n",
        "    * Instead of building one large tree on the entire dataset, Random Forest creates multiple smaller decision trees.\n",
        "    * Each individual tree is trained on a **bootstrap sample** (a random subset of the original training data with replacement). This introduces diversity among the trees.\n",
        "\n",
        "2.  **Feature Randomness:**\n",
        "    * When building each tree, at every split point, the algorithm doesn't consider all available features. Instead, it randomly selects a **subset of features** to choose from. This further decorrelates the trees, making the ensemble more robust.\n",
        "\n",
        "3.  **Ensemble Prediction:**\n",
        "    * For classification, when making a prediction on new data, each individual tree \"votes\" for a class. The Random Forest then aggregates these votes (e.g., by majority vote) to determine the final predicted class.\n",
        "\n",
        "**Advantages for Credit Scoring:**\n",
        "* **Reduced Overfitting:** By averaging predictions from many decorrelated trees, Random Forest significantly reduces variance and overfitting, leading to better generalization.\n",
        "* **Handles Non-linearity:** Decision trees are inherently capable of capturing complex, non-linear relationships and interactions between features.\n",
        "* **Robustness:** Less sensitive to outliers due to the nature of tree-based splits.\n",
        "* **Built-in Feature Importance:** Random Forest provides a direct way to estimate feature importance.\n",
        "\n",
        "**Feature Importance (Conceptual Explanation using Model Explainability)**\n",
        "\n",
        "For Random Forest, feature importance is typically estimated using either **Mean Decrease Impurity (MDI)** (also known as Gini importance) or **Permutation Importance**.\n",
        "\n",
        "* **Mean Decrease Impurity (MDI) / Gini Importance:**\n",
        "    * This method calculates how much each feature contributes to the reduction of impurity (e.g., Gini impurity or entropy) across all decision trees in the forest.\n",
        "    * When a feature is used to split a node, the impurity of the data after the split is lower than before. The more a feature reduces impurity across all splits in all trees, the higher its importance score.\n",
        "    * **Business Indication:** Features with higher Gini importance scores are considered more influential in the model's decision-making process.\n",
        "\n",
        "* **Permutation Importance:** (Often preferred as it's less biased than MDI for some scenarios)\n",
        "    * This method involves shuffling the values of a single feature in the validation set and then measuring how much the model's performance (e.g., accuracy or F1-score) decreases.\n",
        "    * If shuffling a feature significantly drops the model's performance, that feature is considered important.\n",
        "    * **Business Indication:** This directly tells you how much the model *relies* on that specific feature for accurate predictions.\n",
        "\n",
        "**Explanation using an Explainability Tool (Conceptual):**\n",
        "\n",
        "* **SHAP Values:** SHAP values explain the prediction of an instance by computing the contribution of each feature to the prediction. For each individual customer, SHAP could tell us:\n",
        "    * \"This customer was predicted as 'Good' because their `Annual_Income` was high (+X contribution), their `Num_of_Delayed_Payment` was low (+Y contribution), but their `Outstanding_Debt` was moderately high (-Z contribution).\"\n",
        "    * This is incredibly valuable for loan officers to understand *why* a specific credit score was assigned, enabling transparency and trust.\n",
        "    * They can also be aggregated to show **global feature importance**, which would align with the Gini/Permutation importance.\n",
        "\n",
        "* **LIME (Local Interpretable Model-agnostic Explanations):** LIME helps explain individual predictions of any \"black-box\" model (like Random Forest) by locally approximating it with an interpretable model (e.g., a simple linear model).\n",
        "    * **Business Indication:** For a specific loan applicant, LIME could highlight 2-3 most influential features that led to their predicted credit score, helping decision-makers quickly grasp the primary drivers for that particular case.\n",
        "\n",
        "**Inferred Most Important Features for Credit Score Prediction (for Random Forest):**\n",
        "\n",
        "Based on common credit risk factors and the nature of the data, the Random Forest model would likely identify the following features as most important:\n",
        "\n",
        "1.  **Annual_Incom** (or engineered ratios like Debt_to_Income_Ratio / Utilization_Income_Ratio): Directly impacts repayment capacity.\n",
        "2.  **Num_of_Delayed_Payment / Delay_from_due_date / Payment_Consistency:** Direct indicators of past payment behavior and reliability.\n",
        "3.  **Credit_Utilization_Ratio** (and potentially Outstanding_Debt if not dropped due to correlation): Reflects current debt burden relative to available credit, a key indicator of financial strain.\n",
        "4.  **Interest_Rate:** Often a direct proxy of perceived risk, the model would learn its strong inverse relationship with 'Good' credit.\n",
        "5.  **Credit_History_Age** Longer, established credit history generally indicates more reliable borrowing behavior.\n",
        "6.  **Num_Credit_Inquiries:** Frequent inquiries can suggest higher risk or desperate need for credit.\n",
        "7.  **Credit_Mix:** A healthy mix of credit types shows responsible financial management.\n",
        "\n",
        "These features, either individually or in combination, would have significantly contributed to reducing impurity and driving the splits in the many decision trees within the Random Forest, thereby being deemed highly important by the model. This knowledge is gold for Paisabazaar in refining their credit policies and focusing on the most impactful data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Save the File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "# Load the File and predict unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "Through this project, a powerful machine learning model was succesfully develepoed and designed to accurately predict individual credit scores for Paisabazaar. We poured effort into meticulously cleaning and preparing the data, crafting new features, and smartly balancing the dataset to ensure our model learns effectively from all customer profiles.\n",
        "\n",
        "After rigorously implementing and optimizing several algorithms, our Optimized Random Forest Classifier emerged as the clear winner. This model isn't just accurate; it truly excels where it matters most for the business: it's incredibly effective at flagging high-risk 'Poor' credit scores, which helps us significantly reduce potential defaults. Equally important, it's highly reliable in identifying genuinely 'Good' credit scores, allowing us to confidently extend favorable offers to our most creditworthy customers.\n",
        "\n",
        "Ultimately, this means there is now a robust, data-driven tool that will enable Paisabazaar to make smarter, more confident lending decisions, manage risk more effectively, and tailor financial products more precisely. It's a significant step towards boosting both efficiency and profitability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "4Of9eVA-YrdM",
        "F6T5p64dYrdO",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "U2RJ9gkRphqQ",
        "tgIPom80phqQ",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "GBZCJ30FWOIk",
        "3lANiNJUy26A",
        "rsfguXj5y6EF",
        "9LW4FQg2y5iK",
        "nVWLy809WoGk",
        "vRrKeHJzA-F1",
        "jeazSUeyBFNP",
        "CuBd5cnkWwQh",
        "-2ErhMWeJVU-",
        "3Iy9ZCuUJchG",
        "oI8adiFaW2JX",
        "lA81WLqFKP8B",
        "nX0PTzvpKSzZ",
        "BPsdE-NJXbip",
        "CdlC6-8WXcMh",
        "H4tfPVvoK5hc",
        "lfqEmc0UK6PD",
        "btWzTiLzXcbp",
        "Lf9GTRQOLJav",
        "rZp5gpQELKGJ",
        "ghf6b-cVXc5T",
        "0i06UorFLgLp",
        "znUTlpMYLg7q",
        "_hvURAXIXdKx",
        "NUmVAd4HLuO_",
        "NPNJbsHjLvnt",
        "zONjRn9xXzOr",
        "eFsbezTIL72R",
        "DIp_q00kL8jm",
        "WIo51DWGL9O8",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "g-ATYxFrGrvw",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "bn_IUdTipZyH",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "P1XJ9OREExlT",
        "TIqpNgepFxVj",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}